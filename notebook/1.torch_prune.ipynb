{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型剪枝实践\n",
    "Pytorch在1.4.0版本开始，加入了剪枝操作，在torch.nn.utils.prune模块中，本项目按照剪枝范围划分，将其分以下几种剪枝方式:\n",
    "- 局部剪枝（Local Pruning）\n",
    "  - 结构化剪枝\n",
    "    - 随机结构化剪枝（random_structured）\n",
    "    - 范数结构化剪枝（ln_structured）\n",
    "  - 非结构化剪枝\n",
    "    - 随机非结构化剪枝（random_unstructured）\n",
    "    - 范数非结构化剪枝（l1_unstructured）\n",
    "- 全局剪枝（Global Pruning）\n",
    "  - 非结构化剪枝（global_unstructured）\n",
    "- 自定义剪枝（Custom  Pruning）\n",
    "  \n",
    "**注：** 全局剪枝只有非结构化剪枝方式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、局部剪枝\n",
    "首先介绍局部剪枝（Local Pruning）方式，指的是对网络的单个层或局部范围内进行剪枝。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 结构化剪枝\n",
    "按照剪枝方式划分，可以分为结构化剪枝和非结构化剪枝方式。非结构化剪枝会随机地将一些权重参数变为0，结构化剪枝则将某个维度某些通道变成0。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 随机结构化剪枝（random_structured）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建一个经典的LeNet网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个LeNet网络\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(in_features=16 * 4 * 4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=84)\n",
    "        self.fc3 = nn.Linear(in_features=84, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.maxpool(F.relu(self.conv1(x)))\n",
    "        x = self.maxpool(F.relu(self.conv2(x)))\n",
    "\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LeNet().to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 6, 24, 24]             156\n",
      "         MaxPool2d-2            [-1, 6, 12, 12]               0\n",
      "            Conv2d-3             [-1, 16, 8, 8]           2,416\n",
      "         MaxPool2d-4             [-1, 16, 4, 4]               0\n",
      "            Linear-5                  [-1, 120]          30,840\n",
      "            Linear-6                   [-1, 84]          10,164\n",
      "            Linear-7                   [-1, 10]             850\n",
      "================================================================\n",
      "Total params: 44,426\n",
      "Trainable params: 44,426\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.04\n",
      "Params size (MB): 0.17\n",
      "Estimated Total Size (MB): 0.22\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 打印模型结构\n",
    "summary(model, input_size=(1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('weight', Parameter containing:\n",
      "tensor([[[[ 0.0199, -0.1327, -0.1888,  0.0135, -0.1062],\n",
      "          [-0.1044, -0.1658,  0.1823, -0.0305, -0.1942],\n",
      "          [-0.1320,  0.0258, -0.0003, -0.0305, -0.1435],\n",
      "          [-0.0992, -0.0019,  0.1422, -0.1876, -0.1113],\n",
      "          [ 0.0927, -0.0115,  0.1514,  0.0101,  0.1839]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1230, -0.1329,  0.0667,  0.0866, -0.1326],\n",
      "          [ 0.1693,  0.0348,  0.0219,  0.0047, -0.0020],\n",
      "          [ 0.1351, -0.0226,  0.1412, -0.0835,  0.1451],\n",
      "          [-0.1025, -0.0730, -0.0170, -0.0361, -0.0915],\n",
      "          [ 0.0103,  0.0657, -0.0003, -0.1348,  0.1867]]],\n",
      "\n",
      "\n",
      "        [[[-0.0462, -0.1972,  0.1027,  0.0306,  0.0201],\n",
      "          [ 0.1154,  0.1172, -0.0053,  0.1465,  0.1623],\n",
      "          [-0.0238, -0.0572,  0.1679,  0.0517, -0.0271],\n",
      "          [-0.0193, -0.0926, -0.1101,  0.1252,  0.1987],\n",
      "          [-0.1187,  0.0380, -0.1373,  0.1509, -0.1453]]],\n",
      "\n",
      "\n",
      "        [[[-0.1933,  0.1175,  0.1616, -0.0805,  0.0404],\n",
      "          [-0.1942, -0.1721,  0.1894,  0.0864,  0.1413],\n",
      "          [ 0.0250, -0.1376, -0.1769,  0.0968,  0.0714],\n",
      "          [ 0.1608, -0.1090,  0.0120, -0.0126, -0.0472],\n",
      "          [-0.1359,  0.0884,  0.1094, -0.1216, -0.1735]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0564, -0.1212, -0.0545,  0.0416,  0.1363],\n",
      "          [-0.1235,  0.1163, -0.1436,  0.0375,  0.1218],\n",
      "          [-0.0306, -0.1670, -0.0091, -0.0653, -0.1857],\n",
      "          [ 0.0084,  0.0360,  0.0639, -0.0800,  0.0445],\n",
      "          [ 0.1117,  0.1721, -0.1586, -0.1977, -0.1695]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0169,  0.1757, -0.0178, -0.0448,  0.1480],\n",
      "          [-0.0359, -0.1438, -0.0888, -0.0089, -0.1515],\n",
      "          [ 0.1817, -0.0435, -0.0259,  0.0408, -0.0096],\n",
      "          [-0.1641,  0.0599,  0.0107,  0.0912,  0.1782],\n",
      "          [ 0.1733,  0.1339,  0.0162, -0.0256, -0.0540]]]], requires_grad=True)), ('bias', Parameter containing:\n",
      "tensor([ 0.1717, -0.1737, -0.0202,  0.1787,  0.0203, -0.1731],\n",
      "       requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "# 打印第一个卷积层的参数\n",
    "module1 = model.conv1\n",
    "print(list(module1.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# 打印module1中的属性张量named_buffers，初始时为空列表\n",
    "print(list(module1.named_buffers()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])\n"
     ]
    }
   ],
   "source": [
    "# 打印模型的状态字典，状态字典里包含了所有的参数\n",
    "print(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第一个参数: module1, 代表要进行剪枝的特定模块, 这里指的是module=model.conv1,\n",
    "#             说明这里要对第一个卷积层执行剪枝.\n",
    "# 第二个参数: name, 代表要对选中的模块中的哪些参数执行剪枝.\n",
    "#             这里设定为name=\"weight\", 说明是对网络中的weight剪枝, 而不对bias剪枝.\n",
    "# 第三个参数: amount, 代表要对模型中特定比例或绝对数量的参数执行剪枝.\n",
    "#             amount是一个介于0.0-1.0的float数值,代表比例, 或者一个正整数，代表指定剪裁掉多少个参数.\n",
    "# 第四个参数: dim, 代表要进行剪枝通道(channel)的维度索引.\n",
    "#            \n",
    "\n",
    "prune.random_structured(module1, name=\"weight\", amount=0.2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['conv1.bias', 'conv1.weight_orig', 'conv1.weight_mask', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])\n"
     ]
    }
   ],
   "source": [
    "# 再次打印模型的状态字典，观察conv1层\n",
    "print(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bias', Parameter containing:\n",
      "tensor([ 0.1717, -0.1737, -0.0202,  0.1787,  0.0203, -0.1731],\n",
      "       requires_grad=True)), ('weight_orig', Parameter containing:\n",
      "tensor([[[[ 0.0199, -0.1327, -0.1888,  0.0135, -0.1062],\n",
      "          [-0.1044, -0.1658,  0.1823, -0.0305, -0.1942],\n",
      "          [-0.1320,  0.0258, -0.0003, -0.0305, -0.1435],\n",
      "          [-0.0992, -0.0019,  0.1422, -0.1876, -0.1113],\n",
      "          [ 0.0927, -0.0115,  0.1514,  0.0101,  0.1839]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1230, -0.1329,  0.0667,  0.0866, -0.1326],\n",
      "          [ 0.1693,  0.0348,  0.0219,  0.0047, -0.0020],\n",
      "          [ 0.1351, -0.0226,  0.1412, -0.0835,  0.1451],\n",
      "          [-0.1025, -0.0730, -0.0170, -0.0361, -0.0915],\n",
      "          [ 0.0103,  0.0657, -0.0003, -0.1348,  0.1867]]],\n",
      "\n",
      "\n",
      "        [[[-0.0462, -0.1972,  0.1027,  0.0306,  0.0201],\n",
      "          [ 0.1154,  0.1172, -0.0053,  0.1465,  0.1623],\n",
      "          [-0.0238, -0.0572,  0.1679,  0.0517, -0.0271],\n",
      "          [-0.0193, -0.0926, -0.1101,  0.1252,  0.1987],\n",
      "          [-0.1187,  0.0380, -0.1373,  0.1509, -0.1453]]],\n",
      "\n",
      "\n",
      "        [[[-0.1933,  0.1175,  0.1616, -0.0805,  0.0404],\n",
      "          [-0.1942, -0.1721,  0.1894,  0.0864,  0.1413],\n",
      "          [ 0.0250, -0.1376, -0.1769,  0.0968,  0.0714],\n",
      "          [ 0.1608, -0.1090,  0.0120, -0.0126, -0.0472],\n",
      "          [-0.1359,  0.0884,  0.1094, -0.1216, -0.1735]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0564, -0.1212, -0.0545,  0.0416,  0.1363],\n",
      "          [-0.1235,  0.1163, -0.1436,  0.0375,  0.1218],\n",
      "          [-0.0306, -0.1670, -0.0091, -0.0653, -0.1857],\n",
      "          [ 0.0084,  0.0360,  0.0639, -0.0800,  0.0445],\n",
      "          [ 0.1117,  0.1721, -0.1586, -0.1977, -0.1695]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0169,  0.1757, -0.0178, -0.0448,  0.1480],\n",
      "          [-0.0359, -0.1438, -0.0888, -0.0089, -0.1515],\n",
      "          [ 0.1817, -0.0435, -0.0259,  0.0408, -0.0096],\n",
      "          [-0.1641,  0.0599,  0.0107,  0.0912,  0.1782],\n",
      "          [ 0.1733,  0.1339,  0.0162, -0.0256, -0.0540]]]], requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "# 再次打印module1中的属性张量named_buffers\n",
    "print(list(module1.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('weight_mask', tensor([[[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]]]))]\n"
     ]
    }
   ],
   "source": [
    "# 再次打印module1中的属性张量named_buffers\n",
    "print(list(module1.named_buffers()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结论: 经过剪枝操作后, 原始的权重矩阵weight变成了weight_orig. 并且剪枝前打印为空列表的module.named_buffers(), 现在多了weight_mask参数."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.0199, -0.1327, -0.1888,  0.0135, -0.1062],\n",
      "          [-0.1044, -0.1658,  0.1823, -0.0305, -0.1942],\n",
      "          [-0.1320,  0.0258, -0.0003, -0.0305, -0.1435],\n",
      "          [-0.0992, -0.0019,  0.1422, -0.1876, -0.1113],\n",
      "          [ 0.0927, -0.0115,  0.1514,  0.0101,  0.1839]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1230, -0.1329,  0.0667,  0.0866, -0.1326],\n",
      "          [ 0.1693,  0.0348,  0.0219,  0.0047, -0.0020],\n",
      "          [ 0.1351, -0.0226,  0.1412, -0.0835,  0.1451],\n",
      "          [-0.1025, -0.0730, -0.0170, -0.0361, -0.0915],\n",
      "          [ 0.0103,  0.0657, -0.0003, -0.1348,  0.1867]]],\n",
      "\n",
      "\n",
      "        [[[-0.0462, -0.1972,  0.1027,  0.0306,  0.0201],\n",
      "          [ 0.1154,  0.1172, -0.0053,  0.1465,  0.1623],\n",
      "          [-0.0238, -0.0572,  0.1679,  0.0517, -0.0271],\n",
      "          [-0.0193, -0.0926, -0.1101,  0.1252,  0.1987],\n",
      "          [-0.1187,  0.0380, -0.1373,  0.1509, -0.1453]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000, -0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000, -0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0564, -0.1212, -0.0545,  0.0416,  0.1363],\n",
      "          [-0.1235,  0.1163, -0.1436,  0.0375,  0.1218],\n",
      "          [-0.0306, -0.1670, -0.0091, -0.0653, -0.1857],\n",
      "          [ 0.0084,  0.0360,  0.0639, -0.0800,  0.0445],\n",
      "          [ 0.1117,  0.1721, -0.1586, -0.1977, -0.1695]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0169,  0.1757, -0.0178, -0.0448,  0.1480],\n",
      "          [-0.0359, -0.1438, -0.0888, -0.0089, -0.1515],\n",
      "          [ 0.1817, -0.0435, -0.0259,  0.0408, -0.0096],\n",
      "          [-0.1641,  0.0599,  0.0107,  0.0912,  0.1782],\n",
      "          [ 0.1733,  0.1339,  0.0162, -0.0256, -0.0540]]]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 打印module1.weight, 看看发现了什么？\n",
    "print(module1.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结论: 经过剪枝操作后， 原始的weight变成了weight_orig，并存放在named_parameters中, 对应的剪枝矩阵存放在weight_mask中, 将weight_mask视作掩码张量, 再和weight_orig相乘的结果就存放在了weight中."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意:** 剪枝操作后的weight已经不再是module的参数(parameter), 而只是module的一个属性(attribute)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于每一次剪枝操作, 模型都会对应一个具体的_forward_pre_hooks函数用于剪枝，该函数存放执行过的剪枝操作."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([(56, <torch.nn.utils.prune.RandomStructured object at 0x0000026F817EA310>)])\n"
     ]
    }
   ],
   "source": [
    "# 打印_forward_pre_hooks\n",
    "print(module1._forward_pre_hooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 范数结构化剪枝（ln_structured）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['conv1.bias', 'conv1.weight_orig', 'conv1.weight_mask', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])\n",
      "**************************************************\n",
      "[('weight', Parameter containing:\n",
      "tensor([[[[ 0.0210, -0.0655,  0.0230,  0.0613, -0.0271],\n",
      "          [-0.0699,  0.0637, -0.0151, -0.0677,  0.0258],\n",
      "          [ 0.0524,  0.0122,  0.0715,  0.0532,  0.0003],\n",
      "          [-0.0535,  0.0701,  0.0092, -0.0380, -0.0733],\n",
      "          [-0.0700,  0.0787, -0.0032,  0.0699,  0.0131]],\n",
      "\n",
      "         [[-0.0752, -0.0405, -0.0301, -0.0688,  0.0344],\n",
      "          [ 0.0741, -0.0329, -0.0034, -0.0423,  0.0569],\n",
      "          [-0.0753,  0.0278,  0.0411,  0.0265,  0.0094],\n",
      "          [-0.0207,  0.0377, -0.0015,  0.0758, -0.0226],\n",
      "          [-0.0267, -0.0606, -0.0489,  0.0108,  0.0403]],\n",
      "\n",
      "         [[ 0.0536,  0.0718,  0.0239,  0.0475,  0.0764],\n",
      "          [ 0.0763, -0.0240, -0.0424, -0.0742, -0.0398],\n",
      "          [-0.0459, -0.0795,  0.0269,  0.0177, -0.0796],\n",
      "          [ 0.0132,  0.0038,  0.0613, -0.0618,  0.0039],\n",
      "          [-0.0528,  0.0310, -0.0150,  0.0130,  0.0704]],\n",
      "\n",
      "         [[ 0.0786,  0.0597,  0.0314, -0.0299, -0.0053],\n",
      "          [ 0.0206, -0.0619,  0.0023, -0.0568, -0.0475],\n",
      "          [-0.0202,  0.0103,  0.0073,  0.0738, -0.0806],\n",
      "          [ 0.0473,  0.0063, -0.0282,  0.0774,  0.0347],\n",
      "          [ 0.0740, -0.0478, -0.0812,  0.0178, -0.0201]],\n",
      "\n",
      "         [[ 0.0555, -0.0467,  0.0303, -0.0006,  0.0757],\n",
      "          [ 0.0531,  0.0612, -0.0816,  0.0536,  0.0020],\n",
      "          [-0.0483,  0.0280,  0.0397, -0.0647, -0.0102],\n",
      "          [-0.0343, -0.0011,  0.0508,  0.0690,  0.0580],\n",
      "          [-0.0516,  0.0205, -0.0308,  0.0098,  0.0451]],\n",
      "\n",
      "         [[ 0.0092, -0.0552,  0.0511,  0.0166,  0.0659],\n",
      "          [-0.0155, -0.0232, -0.0073, -0.0257,  0.0024],\n",
      "          [-0.0359, -0.0160, -0.0342,  0.0576, -0.0215],\n",
      "          [-0.0741,  0.0204,  0.0146,  0.0103, -0.0683],\n",
      "          [-0.0471,  0.0516, -0.0497, -0.0337, -0.0073]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0652, -0.0267, -0.0183, -0.0653, -0.0779],\n",
      "          [-0.0100, -0.0438, -0.0756,  0.0215,  0.0148],\n",
      "          [-0.0014, -0.0703,  0.0468, -0.0521, -0.0122],\n",
      "          [ 0.0435, -0.0666,  0.0672,  0.0728, -0.0143],\n",
      "          [ 0.0723,  0.0352,  0.0044, -0.0535,  0.0267]],\n",
      "\n",
      "         [[-0.0402,  0.0512,  0.0296, -0.0010,  0.0147],\n",
      "          [-0.0795, -0.0532,  0.0080,  0.0340,  0.0756],\n",
      "          [-0.0013, -0.0749, -0.0349, -0.0768, -0.0809],\n",
      "          [ 0.0172,  0.0747,  0.0251,  0.0209,  0.0239],\n",
      "          [-0.0807,  0.0637,  0.0367, -0.0344, -0.0159]],\n",
      "\n",
      "         [[-0.0132, -0.0470, -0.0320, -0.0109,  0.0712],\n",
      "          [-0.0443,  0.0631, -0.0017, -0.0491,  0.0281],\n",
      "          [-0.0613, -0.0088,  0.0023,  0.0355,  0.0243],\n",
      "          [ 0.0296, -0.0796, -0.0242, -0.0591, -0.0246],\n",
      "          [ 0.0023, -0.0307, -0.0486, -0.0155, -0.0545]],\n",
      "\n",
      "         [[ 0.0459, -0.0765, -0.0045,  0.0528, -0.0658],\n",
      "          [-0.0104, -0.0130, -0.0248, -0.0262, -0.0243],\n",
      "          [-0.0515, -0.0808, -0.0661,  0.0494, -0.0627],\n",
      "          [ 0.0354,  0.0203,  0.0148, -0.0102,  0.0787],\n",
      "          [-0.0246, -0.0421, -0.0385, -0.0378, -0.0395]],\n",
      "\n",
      "         [[ 0.0011,  0.0656,  0.0066,  0.0237, -0.0022],\n",
      "          [-0.0586, -0.0762, -0.0287,  0.0471, -0.0775],\n",
      "          [ 0.0306,  0.0530, -0.0468, -0.0715, -0.0647],\n",
      "          [ 0.0270, -0.0322,  0.0119,  0.0168, -0.0303],\n",
      "          [ 0.0002, -0.0047,  0.0784, -0.0101,  0.0780]],\n",
      "\n",
      "         [[ 0.0657,  0.0159, -0.0117, -0.0725, -0.0753],\n",
      "          [-0.0647, -0.0709,  0.0305,  0.0002, -0.0104],\n",
      "          [ 0.0098, -0.0039,  0.0097,  0.0028,  0.0651],\n",
      "          [ 0.0732, -0.0411,  0.0619,  0.0427,  0.0620],\n",
      "          [ 0.0504,  0.0451,  0.0587, -0.0458, -0.0798]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0559,  0.0492,  0.0388,  0.0542, -0.0814],\n",
      "          [-0.0365,  0.0401, -0.0658, -0.0810,  0.0353],\n",
      "          [ 0.0737, -0.0081,  0.0763,  0.0561,  0.0160],\n",
      "          [-0.0181,  0.0188,  0.0629, -0.0259,  0.0242],\n",
      "          [-0.0256,  0.0562,  0.0720,  0.0106, -0.0373]],\n",
      "\n",
      "         [[ 0.0516, -0.0422, -0.0023, -0.0166,  0.0673],\n",
      "          [ 0.0013, -0.0526, -0.0749,  0.0095,  0.0490],\n",
      "          [ 0.0230,  0.0552, -0.0571,  0.0723, -0.0814],\n",
      "          [-0.0609, -0.0071,  0.0410,  0.0694,  0.0672],\n",
      "          [-0.0318, -0.0409,  0.0347, -0.0686,  0.0765]],\n",
      "\n",
      "         [[ 0.0672, -0.0291, -0.0279,  0.0005,  0.0339],\n",
      "          [ 0.0677,  0.0636,  0.0213,  0.0553, -0.0710],\n",
      "          [ 0.0007, -0.0744, -0.0631, -0.0777,  0.0183],\n",
      "          [ 0.0068, -0.0553,  0.0073,  0.0252, -0.0673],\n",
      "          [ 0.0710, -0.0498, -0.0651,  0.0157,  0.0180]],\n",
      "\n",
      "         [[ 0.0773,  0.0151, -0.0743,  0.0483, -0.0764],\n",
      "          [ 0.0725, -0.0793,  0.0067,  0.0451, -0.0092],\n",
      "          [ 0.0275,  0.0100, -0.0205,  0.0734, -0.0546],\n",
      "          [ 0.0158, -0.0079, -0.0035, -0.0574,  0.0225],\n",
      "          [-0.0748,  0.0701,  0.0767,  0.0233,  0.0182]],\n",
      "\n",
      "         [[ 0.0201, -0.0202, -0.0810,  0.0795, -0.0413],\n",
      "          [ 0.0417,  0.0723,  0.0219, -0.0803,  0.0016],\n",
      "          [-0.0394,  0.0135, -0.0815, -0.0241,  0.0624],\n",
      "          [ 0.0188, -0.0383,  0.0652, -0.0303, -0.0519],\n",
      "          [ 0.0257,  0.0468,  0.0176, -0.0253, -0.0664]],\n",
      "\n",
      "         [[ 0.0560,  0.0654,  0.0417,  0.0094, -0.0723],\n",
      "          [ 0.0185,  0.0606, -0.0114,  0.0620,  0.0195],\n",
      "          [-0.0335, -0.0420, -0.0812,  0.0057,  0.0336],\n",
      "          [ 0.0392,  0.0545, -0.0300,  0.0176, -0.0542],\n",
      "          [ 0.0534,  0.0194,  0.0053, -0.0077, -0.0411]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0739, -0.0597,  0.0324, -0.0503,  0.0103],\n",
      "          [ 0.0146, -0.0641,  0.0324, -0.0616, -0.0800],\n",
      "          [-0.0460, -0.0610,  0.0163,  0.0245, -0.0757],\n",
      "          [-0.0457,  0.0258,  0.0579,  0.0705,  0.0168],\n",
      "          [-0.0320, -0.0027, -0.0602, -0.0130, -0.0774]],\n",
      "\n",
      "         [[-0.0560,  0.0280,  0.0215, -0.0381,  0.0589],\n",
      "          [-0.0312, -0.0798,  0.0631, -0.0466,  0.0094],\n",
      "          [ 0.0094, -0.0468,  0.0105, -0.0804, -0.0553],\n",
      "          [ 0.0139,  0.0591,  0.0766,  0.0014, -0.0195],\n",
      "          [-0.0246, -0.0200,  0.0153,  0.0370, -0.0800]],\n",
      "\n",
      "         [[ 0.0534, -0.0152, -0.0584, -0.0494, -0.0177],\n",
      "          [-0.0107, -0.0525, -0.0580, -0.0676,  0.0162],\n",
      "          [ 0.0353, -0.0589,  0.0608, -0.0002, -0.0350],\n",
      "          [-0.0743, -0.0578, -0.0307, -0.0712, -0.0542],\n",
      "          [ 0.0234, -0.0624,  0.0503, -0.0715,  0.0622]],\n",
      "\n",
      "         [[-0.0389, -0.0660,  0.0736,  0.0207,  0.0794],\n",
      "          [ 0.0559, -0.0656, -0.0191,  0.0537,  0.0393],\n",
      "          [-0.0276, -0.0658,  0.0182, -0.0128, -0.0110],\n",
      "          [ 0.0562, -0.0031, -0.0124,  0.0015, -0.0351],\n",
      "          [-0.0156, -0.0370, -0.0549,  0.0583, -0.0428]],\n",
      "\n",
      "         [[ 0.0486,  0.0647, -0.0271,  0.0373, -0.0008],\n",
      "          [-0.0232,  0.0253, -0.0492,  0.0100,  0.0092],\n",
      "          [-0.0642, -0.0553, -0.0727,  0.0077,  0.0599],\n",
      "          [ 0.0290,  0.0393, -0.0497,  0.0324, -0.0465],\n",
      "          [ 0.0403,  0.0674, -0.0202,  0.0417, -0.0778]],\n",
      "\n",
      "         [[ 0.0748, -0.0753,  0.0221, -0.0428, -0.0701],\n",
      "          [-0.0751, -0.0249, -0.0312,  0.0116,  0.0385],\n",
      "          [ 0.0680,  0.0581,  0.0261, -0.0680, -0.0372],\n",
      "          [-0.0569,  0.0429,  0.0015,  0.0672, -0.0064],\n",
      "          [ 0.0148, -0.0391, -0.0159,  0.0684, -0.0157]]],\n",
      "\n",
      "\n",
      "        [[[-0.0698,  0.0498,  0.0617, -0.0567, -0.0224],\n",
      "          [ 0.0683, -0.0263, -0.0275, -0.0735, -0.0638],\n",
      "          [-0.0020,  0.0425,  0.0723, -0.0717,  0.0280],\n",
      "          [-0.0694, -0.0267,  0.0601,  0.0559,  0.0180],\n",
      "          [-0.0650,  0.0703, -0.0290,  0.0781, -0.0421]],\n",
      "\n",
      "         [[-0.0757,  0.0235, -0.0789, -0.0036, -0.0312],\n",
      "          [-0.0778,  0.0093,  0.0471, -0.0294,  0.0573],\n",
      "          [-0.0747, -0.0321,  0.0113,  0.0495,  0.0085],\n",
      "          [ 0.0255,  0.0034,  0.0144, -0.0470, -0.0412],\n",
      "          [-0.0364, -0.0755, -0.0489,  0.0579, -0.0802]],\n",
      "\n",
      "         [[-0.0126,  0.0376,  0.0737, -0.0560, -0.0185],\n",
      "          [-0.0495,  0.0070,  0.0509,  0.0508, -0.0756],\n",
      "          [ 0.0002,  0.0671,  0.0800, -0.0073, -0.0522],\n",
      "          [-0.0398,  0.0015,  0.0715,  0.0803,  0.0710],\n",
      "          [ 0.0561, -0.0080, -0.0184, -0.0517,  0.0724]],\n",
      "\n",
      "         [[ 0.0208, -0.0199,  0.0538, -0.0791,  0.0495],\n",
      "          [ 0.0435,  0.0508,  0.0464, -0.0514, -0.0008],\n",
      "          [ 0.0764, -0.0591, -0.0398,  0.0304,  0.0228],\n",
      "          [ 0.0394,  0.0475, -0.0498,  0.0499,  0.0016],\n",
      "          [ 0.0639, -0.0362,  0.0791,  0.0512,  0.0384]],\n",
      "\n",
      "         [[ 0.0808, -0.0379,  0.0736, -0.0789,  0.0295],\n",
      "          [ 0.0348, -0.0021,  0.0443,  0.0224, -0.0783],\n",
      "          [ 0.0468,  0.0560,  0.0079,  0.0634, -0.0435],\n",
      "          [ 0.0232,  0.0183, -0.0477, -0.0220,  0.0160],\n",
      "          [ 0.0789,  0.0415,  0.0302, -0.0185, -0.0110]],\n",
      "\n",
      "         [[-0.0030,  0.0791,  0.0655,  0.0116, -0.0799],\n",
      "          [-0.0360, -0.0101, -0.0563,  0.0666,  0.0093],\n",
      "          [ 0.0684, -0.0494,  0.0016,  0.0764, -0.0245],\n",
      "          [-0.0404, -0.0220, -0.0585, -0.0017,  0.0018],\n",
      "          [-0.0710, -0.0269,  0.0673, -0.0553,  0.0636]]],\n",
      "\n",
      "\n",
      "        [[[-0.0189, -0.0397, -0.0537,  0.0572, -0.0166],\n",
      "          [-0.0627, -0.0212,  0.0554,  0.0048, -0.0260],\n",
      "          [ 0.0794, -0.0020,  0.0640, -0.0155, -0.0757],\n",
      "          [-0.0091, -0.0801, -0.0686, -0.0642,  0.0196],\n",
      "          [-0.0401, -0.0456, -0.0477,  0.0311,  0.0735]],\n",
      "\n",
      "         [[ 0.0164, -0.0159, -0.0684, -0.0661, -0.0529],\n",
      "          [ 0.0702,  0.0050,  0.0620, -0.0067, -0.0797],\n",
      "          [-0.0215, -0.0789, -0.0111,  0.0056,  0.0418],\n",
      "          [-0.0770,  0.0384,  0.0653,  0.0071,  0.0507],\n",
      "          [ 0.0579, -0.0558, -0.0258,  0.0396, -0.0060]],\n",
      "\n",
      "         [[ 0.0033,  0.0067,  0.0393, -0.0316,  0.0799],\n",
      "          [-0.0205,  0.0060, -0.0222, -0.0310, -0.0629],\n",
      "          [ 0.0343,  0.0277,  0.0024,  0.0407,  0.0445],\n",
      "          [ 0.0170,  0.0093, -0.0446,  0.0575, -0.0074],\n",
      "          [-0.0554, -0.0528, -0.0224, -0.0017, -0.0812]],\n",
      "\n",
      "         [[-0.0561,  0.0294, -0.0583, -0.0631,  0.0549],\n",
      "          [ 0.0622, -0.0442,  0.0435,  0.0288, -0.0514],\n",
      "          [-0.0096, -0.0090,  0.0471,  0.0023, -0.0467],\n",
      "          [-0.0385,  0.0377, -0.0137,  0.0197,  0.0200],\n",
      "          [-0.0467, -0.0666,  0.0683, -0.0691, -0.0816]],\n",
      "\n",
      "         [[-0.0580,  0.0210,  0.0562,  0.0806, -0.0633],\n",
      "          [-0.0254,  0.0311, -0.0040,  0.0196, -0.0769],\n",
      "          [ 0.0722, -0.0307,  0.0239,  0.0779, -0.0806],\n",
      "          [ 0.0761, -0.0642,  0.0608,  0.0753,  0.0139],\n",
      "          [ 0.0395,  0.0764,  0.0288,  0.0062,  0.0695]],\n",
      "\n",
      "         [[ 0.0336,  0.0123, -0.0197,  0.0436, -0.0173],\n",
      "          [ 0.0475, -0.0394, -0.0055, -0.0014, -0.0372],\n",
      "          [ 0.0028, -0.0401,  0.0359,  0.0792,  0.0448],\n",
      "          [ 0.0387, -0.0750, -0.0501, -0.0058, -0.0208],\n",
      "          [ 0.0697, -0.0622,  0.0607,  0.0653,  0.0745]]]], requires_grad=True)), ('bias', Parameter containing:\n",
      "tensor([-0.0397, -0.0521, -0.0037, -0.0105,  0.0705, -0.0071,  0.0337,  0.0399,\n",
      "         0.0788, -0.0610,  0.0180,  0.0376, -0.0044,  0.0465,  0.0462, -0.0157],\n",
      "       requires_grad=True))]\n",
      "**************************************************\n",
      "[]\n",
      "**************************************************\n",
      "Parameter containing:\n",
      "tensor([-0.0397, -0.0521, -0.0037, -0.0105,  0.0705, -0.0071,  0.0337,  0.0399,\n",
      "         0.0788, -0.0610,  0.0180,  0.0376, -0.0044,  0.0465,  0.0462, -0.0157],\n",
      "       requires_grad=True)\n",
      "**************************************************\n",
      "OrderedDict()\n"
     ]
    }
   ],
   "source": [
    "# 对conv2进行范数结构化剪枝\n",
    "module2 = model.conv2\n",
    "# 再次打印模型参数\n",
    "print(model.state_dict().keys())\n",
    "print('*'*50)\n",
    "print(list(module2.named_parameters()))\n",
    "print('*'*50)\n",
    "print(list(module2.named_buffers()))\n",
    "print('*'*50)\n",
    "print(module2.bias)\n",
    "print('*'*50)\n",
    "print(module2._forward_pre_hooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['conv1.bias', 'conv1.weight_orig', 'conv1.weight_mask', 'conv2.bias', 'conv2.weight_orig', 'conv2.weight_mask', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])\n",
      "**************************************************\n",
      "[('bias', Parameter containing:\n",
      "tensor([-0.0397, -0.0521, -0.0037, -0.0105,  0.0705, -0.0071,  0.0337,  0.0399,\n",
      "         0.0788, -0.0610,  0.0180,  0.0376, -0.0044,  0.0465,  0.0462, -0.0157],\n",
      "       requires_grad=True)), ('weight_orig', Parameter containing:\n",
      "tensor([[[[ 0.0210, -0.0655,  0.0230,  0.0613, -0.0271],\n",
      "          [-0.0699,  0.0637, -0.0151, -0.0677,  0.0258],\n",
      "          [ 0.0524,  0.0122,  0.0715,  0.0532,  0.0003],\n",
      "          [-0.0535,  0.0701,  0.0092, -0.0380, -0.0733],\n",
      "          [-0.0700,  0.0787, -0.0032,  0.0699,  0.0131]],\n",
      "\n",
      "         [[-0.0752, -0.0405, -0.0301, -0.0688,  0.0344],\n",
      "          [ 0.0741, -0.0329, -0.0034, -0.0423,  0.0569],\n",
      "          [-0.0753,  0.0278,  0.0411,  0.0265,  0.0094],\n",
      "          [-0.0207,  0.0377, -0.0015,  0.0758, -0.0226],\n",
      "          [-0.0267, -0.0606, -0.0489,  0.0108,  0.0403]],\n",
      "\n",
      "         [[ 0.0536,  0.0718,  0.0239,  0.0475,  0.0764],\n",
      "          [ 0.0763, -0.0240, -0.0424, -0.0742, -0.0398],\n",
      "          [-0.0459, -0.0795,  0.0269,  0.0177, -0.0796],\n",
      "          [ 0.0132,  0.0038,  0.0613, -0.0618,  0.0039],\n",
      "          [-0.0528,  0.0310, -0.0150,  0.0130,  0.0704]],\n",
      "\n",
      "         [[ 0.0786,  0.0597,  0.0314, -0.0299, -0.0053],\n",
      "          [ 0.0206, -0.0619,  0.0023, -0.0568, -0.0475],\n",
      "          [-0.0202,  0.0103,  0.0073,  0.0738, -0.0806],\n",
      "          [ 0.0473,  0.0063, -0.0282,  0.0774,  0.0347],\n",
      "          [ 0.0740, -0.0478, -0.0812,  0.0178, -0.0201]],\n",
      "\n",
      "         [[ 0.0555, -0.0467,  0.0303, -0.0006,  0.0757],\n",
      "          [ 0.0531,  0.0612, -0.0816,  0.0536,  0.0020],\n",
      "          [-0.0483,  0.0280,  0.0397, -0.0647, -0.0102],\n",
      "          [-0.0343, -0.0011,  0.0508,  0.0690,  0.0580],\n",
      "          [-0.0516,  0.0205, -0.0308,  0.0098,  0.0451]],\n",
      "\n",
      "         [[ 0.0092, -0.0552,  0.0511,  0.0166,  0.0659],\n",
      "          [-0.0155, -0.0232, -0.0073, -0.0257,  0.0024],\n",
      "          [-0.0359, -0.0160, -0.0342,  0.0576, -0.0215],\n",
      "          [-0.0741,  0.0204,  0.0146,  0.0103, -0.0683],\n",
      "          [-0.0471,  0.0516, -0.0497, -0.0337, -0.0073]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0652, -0.0267, -0.0183, -0.0653, -0.0779],\n",
      "          [-0.0100, -0.0438, -0.0756,  0.0215,  0.0148],\n",
      "          [-0.0014, -0.0703,  0.0468, -0.0521, -0.0122],\n",
      "          [ 0.0435, -0.0666,  0.0672,  0.0728, -0.0143],\n",
      "          [ 0.0723,  0.0352,  0.0044, -0.0535,  0.0267]],\n",
      "\n",
      "         [[-0.0402,  0.0512,  0.0296, -0.0010,  0.0147],\n",
      "          [-0.0795, -0.0532,  0.0080,  0.0340,  0.0756],\n",
      "          [-0.0013, -0.0749, -0.0349, -0.0768, -0.0809],\n",
      "          [ 0.0172,  0.0747,  0.0251,  0.0209,  0.0239],\n",
      "          [-0.0807,  0.0637,  0.0367, -0.0344, -0.0159]],\n",
      "\n",
      "         [[-0.0132, -0.0470, -0.0320, -0.0109,  0.0712],\n",
      "          [-0.0443,  0.0631, -0.0017, -0.0491,  0.0281],\n",
      "          [-0.0613, -0.0088,  0.0023,  0.0355,  0.0243],\n",
      "          [ 0.0296, -0.0796, -0.0242, -0.0591, -0.0246],\n",
      "          [ 0.0023, -0.0307, -0.0486, -0.0155, -0.0545]],\n",
      "\n",
      "         [[ 0.0459, -0.0765, -0.0045,  0.0528, -0.0658],\n",
      "          [-0.0104, -0.0130, -0.0248, -0.0262, -0.0243],\n",
      "          [-0.0515, -0.0808, -0.0661,  0.0494, -0.0627],\n",
      "          [ 0.0354,  0.0203,  0.0148, -0.0102,  0.0787],\n",
      "          [-0.0246, -0.0421, -0.0385, -0.0378, -0.0395]],\n",
      "\n",
      "         [[ 0.0011,  0.0656,  0.0066,  0.0237, -0.0022],\n",
      "          [-0.0586, -0.0762, -0.0287,  0.0471, -0.0775],\n",
      "          [ 0.0306,  0.0530, -0.0468, -0.0715, -0.0647],\n",
      "          [ 0.0270, -0.0322,  0.0119,  0.0168, -0.0303],\n",
      "          [ 0.0002, -0.0047,  0.0784, -0.0101,  0.0780]],\n",
      "\n",
      "         [[ 0.0657,  0.0159, -0.0117, -0.0725, -0.0753],\n",
      "          [-0.0647, -0.0709,  0.0305,  0.0002, -0.0104],\n",
      "          [ 0.0098, -0.0039,  0.0097,  0.0028,  0.0651],\n",
      "          [ 0.0732, -0.0411,  0.0619,  0.0427,  0.0620],\n",
      "          [ 0.0504,  0.0451,  0.0587, -0.0458, -0.0798]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0559,  0.0492,  0.0388,  0.0542, -0.0814],\n",
      "          [-0.0365,  0.0401, -0.0658, -0.0810,  0.0353],\n",
      "          [ 0.0737, -0.0081,  0.0763,  0.0561,  0.0160],\n",
      "          [-0.0181,  0.0188,  0.0629, -0.0259,  0.0242],\n",
      "          [-0.0256,  0.0562,  0.0720,  0.0106, -0.0373]],\n",
      "\n",
      "         [[ 0.0516, -0.0422, -0.0023, -0.0166,  0.0673],\n",
      "          [ 0.0013, -0.0526, -0.0749,  0.0095,  0.0490],\n",
      "          [ 0.0230,  0.0552, -0.0571,  0.0723, -0.0814],\n",
      "          [-0.0609, -0.0071,  0.0410,  0.0694,  0.0672],\n",
      "          [-0.0318, -0.0409,  0.0347, -0.0686,  0.0765]],\n",
      "\n",
      "         [[ 0.0672, -0.0291, -0.0279,  0.0005,  0.0339],\n",
      "          [ 0.0677,  0.0636,  0.0213,  0.0553, -0.0710],\n",
      "          [ 0.0007, -0.0744, -0.0631, -0.0777,  0.0183],\n",
      "          [ 0.0068, -0.0553,  0.0073,  0.0252, -0.0673],\n",
      "          [ 0.0710, -0.0498, -0.0651,  0.0157,  0.0180]],\n",
      "\n",
      "         [[ 0.0773,  0.0151, -0.0743,  0.0483, -0.0764],\n",
      "          [ 0.0725, -0.0793,  0.0067,  0.0451, -0.0092],\n",
      "          [ 0.0275,  0.0100, -0.0205,  0.0734, -0.0546],\n",
      "          [ 0.0158, -0.0079, -0.0035, -0.0574,  0.0225],\n",
      "          [-0.0748,  0.0701,  0.0767,  0.0233,  0.0182]],\n",
      "\n",
      "         [[ 0.0201, -0.0202, -0.0810,  0.0795, -0.0413],\n",
      "          [ 0.0417,  0.0723,  0.0219, -0.0803,  0.0016],\n",
      "          [-0.0394,  0.0135, -0.0815, -0.0241,  0.0624],\n",
      "          [ 0.0188, -0.0383,  0.0652, -0.0303, -0.0519],\n",
      "          [ 0.0257,  0.0468,  0.0176, -0.0253, -0.0664]],\n",
      "\n",
      "         [[ 0.0560,  0.0654,  0.0417,  0.0094, -0.0723],\n",
      "          [ 0.0185,  0.0606, -0.0114,  0.0620,  0.0195],\n",
      "          [-0.0335, -0.0420, -0.0812,  0.0057,  0.0336],\n",
      "          [ 0.0392,  0.0545, -0.0300,  0.0176, -0.0542],\n",
      "          [ 0.0534,  0.0194,  0.0053, -0.0077, -0.0411]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0739, -0.0597,  0.0324, -0.0503,  0.0103],\n",
      "          [ 0.0146, -0.0641,  0.0324, -0.0616, -0.0800],\n",
      "          [-0.0460, -0.0610,  0.0163,  0.0245, -0.0757],\n",
      "          [-0.0457,  0.0258,  0.0579,  0.0705,  0.0168],\n",
      "          [-0.0320, -0.0027, -0.0602, -0.0130, -0.0774]],\n",
      "\n",
      "         [[-0.0560,  0.0280,  0.0215, -0.0381,  0.0589],\n",
      "          [-0.0312, -0.0798,  0.0631, -0.0466,  0.0094],\n",
      "          [ 0.0094, -0.0468,  0.0105, -0.0804, -0.0553],\n",
      "          [ 0.0139,  0.0591,  0.0766,  0.0014, -0.0195],\n",
      "          [-0.0246, -0.0200,  0.0153,  0.0370, -0.0800]],\n",
      "\n",
      "         [[ 0.0534, -0.0152, -0.0584, -0.0494, -0.0177],\n",
      "          [-0.0107, -0.0525, -0.0580, -0.0676,  0.0162],\n",
      "          [ 0.0353, -0.0589,  0.0608, -0.0002, -0.0350],\n",
      "          [-0.0743, -0.0578, -0.0307, -0.0712, -0.0542],\n",
      "          [ 0.0234, -0.0624,  0.0503, -0.0715,  0.0622]],\n",
      "\n",
      "         [[-0.0389, -0.0660,  0.0736,  0.0207,  0.0794],\n",
      "          [ 0.0559, -0.0656, -0.0191,  0.0537,  0.0393],\n",
      "          [-0.0276, -0.0658,  0.0182, -0.0128, -0.0110],\n",
      "          [ 0.0562, -0.0031, -0.0124,  0.0015, -0.0351],\n",
      "          [-0.0156, -0.0370, -0.0549,  0.0583, -0.0428]],\n",
      "\n",
      "         [[ 0.0486,  0.0647, -0.0271,  0.0373, -0.0008],\n",
      "          [-0.0232,  0.0253, -0.0492,  0.0100,  0.0092],\n",
      "          [-0.0642, -0.0553, -0.0727,  0.0077,  0.0599],\n",
      "          [ 0.0290,  0.0393, -0.0497,  0.0324, -0.0465],\n",
      "          [ 0.0403,  0.0674, -0.0202,  0.0417, -0.0778]],\n",
      "\n",
      "         [[ 0.0748, -0.0753,  0.0221, -0.0428, -0.0701],\n",
      "          [-0.0751, -0.0249, -0.0312,  0.0116,  0.0385],\n",
      "          [ 0.0680,  0.0581,  0.0261, -0.0680, -0.0372],\n",
      "          [-0.0569,  0.0429,  0.0015,  0.0672, -0.0064],\n",
      "          [ 0.0148, -0.0391, -0.0159,  0.0684, -0.0157]]],\n",
      "\n",
      "\n",
      "        [[[-0.0698,  0.0498,  0.0617, -0.0567, -0.0224],\n",
      "          [ 0.0683, -0.0263, -0.0275, -0.0735, -0.0638],\n",
      "          [-0.0020,  0.0425,  0.0723, -0.0717,  0.0280],\n",
      "          [-0.0694, -0.0267,  0.0601,  0.0559,  0.0180],\n",
      "          [-0.0650,  0.0703, -0.0290,  0.0781, -0.0421]],\n",
      "\n",
      "         [[-0.0757,  0.0235, -0.0789, -0.0036, -0.0312],\n",
      "          [-0.0778,  0.0093,  0.0471, -0.0294,  0.0573],\n",
      "          [-0.0747, -0.0321,  0.0113,  0.0495,  0.0085],\n",
      "          [ 0.0255,  0.0034,  0.0144, -0.0470, -0.0412],\n",
      "          [-0.0364, -0.0755, -0.0489,  0.0579, -0.0802]],\n",
      "\n",
      "         [[-0.0126,  0.0376,  0.0737, -0.0560, -0.0185],\n",
      "          [-0.0495,  0.0070,  0.0509,  0.0508, -0.0756],\n",
      "          [ 0.0002,  0.0671,  0.0800, -0.0073, -0.0522],\n",
      "          [-0.0398,  0.0015,  0.0715,  0.0803,  0.0710],\n",
      "          [ 0.0561, -0.0080, -0.0184, -0.0517,  0.0724]],\n",
      "\n",
      "         [[ 0.0208, -0.0199,  0.0538, -0.0791,  0.0495],\n",
      "          [ 0.0435,  0.0508,  0.0464, -0.0514, -0.0008],\n",
      "          [ 0.0764, -0.0591, -0.0398,  0.0304,  0.0228],\n",
      "          [ 0.0394,  0.0475, -0.0498,  0.0499,  0.0016],\n",
      "          [ 0.0639, -0.0362,  0.0791,  0.0512,  0.0384]],\n",
      "\n",
      "         [[ 0.0808, -0.0379,  0.0736, -0.0789,  0.0295],\n",
      "          [ 0.0348, -0.0021,  0.0443,  0.0224, -0.0783],\n",
      "          [ 0.0468,  0.0560,  0.0079,  0.0634, -0.0435],\n",
      "          [ 0.0232,  0.0183, -0.0477, -0.0220,  0.0160],\n",
      "          [ 0.0789,  0.0415,  0.0302, -0.0185, -0.0110]],\n",
      "\n",
      "         [[-0.0030,  0.0791,  0.0655,  0.0116, -0.0799],\n",
      "          [-0.0360, -0.0101, -0.0563,  0.0666,  0.0093],\n",
      "          [ 0.0684, -0.0494,  0.0016,  0.0764, -0.0245],\n",
      "          [-0.0404, -0.0220, -0.0585, -0.0017,  0.0018],\n",
      "          [-0.0710, -0.0269,  0.0673, -0.0553,  0.0636]]],\n",
      "\n",
      "\n",
      "        [[[-0.0189, -0.0397, -0.0537,  0.0572, -0.0166],\n",
      "          [-0.0627, -0.0212,  0.0554,  0.0048, -0.0260],\n",
      "          [ 0.0794, -0.0020,  0.0640, -0.0155, -0.0757],\n",
      "          [-0.0091, -0.0801, -0.0686, -0.0642,  0.0196],\n",
      "          [-0.0401, -0.0456, -0.0477,  0.0311,  0.0735]],\n",
      "\n",
      "         [[ 0.0164, -0.0159, -0.0684, -0.0661, -0.0529],\n",
      "          [ 0.0702,  0.0050,  0.0620, -0.0067, -0.0797],\n",
      "          [-0.0215, -0.0789, -0.0111,  0.0056,  0.0418],\n",
      "          [-0.0770,  0.0384,  0.0653,  0.0071,  0.0507],\n",
      "          [ 0.0579, -0.0558, -0.0258,  0.0396, -0.0060]],\n",
      "\n",
      "         [[ 0.0033,  0.0067,  0.0393, -0.0316,  0.0799],\n",
      "          [-0.0205,  0.0060, -0.0222, -0.0310, -0.0629],\n",
      "          [ 0.0343,  0.0277,  0.0024,  0.0407,  0.0445],\n",
      "          [ 0.0170,  0.0093, -0.0446,  0.0575, -0.0074],\n",
      "          [-0.0554, -0.0528, -0.0224, -0.0017, -0.0812]],\n",
      "\n",
      "         [[-0.0561,  0.0294, -0.0583, -0.0631,  0.0549],\n",
      "          [ 0.0622, -0.0442,  0.0435,  0.0288, -0.0514],\n",
      "          [-0.0096, -0.0090,  0.0471,  0.0023, -0.0467],\n",
      "          [-0.0385,  0.0377, -0.0137,  0.0197,  0.0200],\n",
      "          [-0.0467, -0.0666,  0.0683, -0.0691, -0.0816]],\n",
      "\n",
      "         [[-0.0580,  0.0210,  0.0562,  0.0806, -0.0633],\n",
      "          [-0.0254,  0.0311, -0.0040,  0.0196, -0.0769],\n",
      "          [ 0.0722, -0.0307,  0.0239,  0.0779, -0.0806],\n",
      "          [ 0.0761, -0.0642,  0.0608,  0.0753,  0.0139],\n",
      "          [ 0.0395,  0.0764,  0.0288,  0.0062,  0.0695]],\n",
      "\n",
      "         [[ 0.0336,  0.0123, -0.0197,  0.0436, -0.0173],\n",
      "          [ 0.0475, -0.0394, -0.0055, -0.0014, -0.0372],\n",
      "          [ 0.0028, -0.0401,  0.0359,  0.0792,  0.0448],\n",
      "          [ 0.0387, -0.0750, -0.0501, -0.0058, -0.0208],\n",
      "          [ 0.0697, -0.0622,  0.0607,  0.0653,  0.0745]]]], requires_grad=True))]\n",
      "**************************************************\n",
      "[('weight_mask', tensor([[[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]]]))]\n",
      "**************************************************\n",
      "tensor([[[[ 0.0210, -0.0655,  0.0230,  0.0613, -0.0271],\n",
      "          [-0.0699,  0.0637, -0.0151, -0.0677,  0.0258],\n",
      "          [ 0.0524,  0.0122,  0.0715,  0.0532,  0.0003],\n",
      "          [-0.0535,  0.0701,  0.0092, -0.0380, -0.0733],\n",
      "          [-0.0700,  0.0787, -0.0032,  0.0699,  0.0131]],\n",
      "\n",
      "         [[-0.0752, -0.0405, -0.0301, -0.0688,  0.0344],\n",
      "          [ 0.0741, -0.0329, -0.0034, -0.0423,  0.0569],\n",
      "          [-0.0753,  0.0278,  0.0411,  0.0265,  0.0094],\n",
      "          [-0.0207,  0.0377, -0.0015,  0.0758, -0.0226],\n",
      "          [-0.0267, -0.0606, -0.0489,  0.0108,  0.0403]],\n",
      "\n",
      "         [[ 0.0536,  0.0718,  0.0239,  0.0475,  0.0764],\n",
      "          [ 0.0763, -0.0240, -0.0424, -0.0742, -0.0398],\n",
      "          [-0.0459, -0.0795,  0.0269,  0.0177, -0.0796],\n",
      "          [ 0.0132,  0.0038,  0.0613, -0.0618,  0.0039],\n",
      "          [-0.0528,  0.0310, -0.0150,  0.0130,  0.0704]],\n",
      "\n",
      "         [[ 0.0786,  0.0597,  0.0314, -0.0299, -0.0053],\n",
      "          [ 0.0206, -0.0619,  0.0023, -0.0568, -0.0475],\n",
      "          [-0.0202,  0.0103,  0.0073,  0.0738, -0.0806],\n",
      "          [ 0.0473,  0.0063, -0.0282,  0.0774,  0.0347],\n",
      "          [ 0.0740, -0.0478, -0.0812,  0.0178, -0.0201]],\n",
      "\n",
      "         [[ 0.0555, -0.0467,  0.0303, -0.0006,  0.0757],\n",
      "          [ 0.0531,  0.0612, -0.0816,  0.0536,  0.0020],\n",
      "          [-0.0483,  0.0280,  0.0397, -0.0647, -0.0102],\n",
      "          [-0.0343, -0.0011,  0.0508,  0.0690,  0.0580],\n",
      "          [-0.0516,  0.0205, -0.0308,  0.0098,  0.0451]],\n",
      "\n",
      "         [[ 0.0092, -0.0552,  0.0511,  0.0166,  0.0659],\n",
      "          [-0.0155, -0.0232, -0.0073, -0.0257,  0.0024],\n",
      "          [-0.0359, -0.0160, -0.0342,  0.0576, -0.0215],\n",
      "          [-0.0741,  0.0204,  0.0146,  0.0103, -0.0683],\n",
      "          [-0.0471,  0.0516, -0.0497, -0.0337, -0.0073]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0652, -0.0267, -0.0183, -0.0653, -0.0779],\n",
      "          [-0.0100, -0.0438, -0.0756,  0.0215,  0.0148],\n",
      "          [-0.0014, -0.0703,  0.0468, -0.0521, -0.0122],\n",
      "          [ 0.0435, -0.0666,  0.0672,  0.0728, -0.0143],\n",
      "          [ 0.0723,  0.0352,  0.0044, -0.0535,  0.0267]],\n",
      "\n",
      "         [[-0.0402,  0.0512,  0.0296, -0.0010,  0.0147],\n",
      "          [-0.0795, -0.0532,  0.0080,  0.0340,  0.0756],\n",
      "          [-0.0013, -0.0749, -0.0349, -0.0768, -0.0809],\n",
      "          [ 0.0172,  0.0747,  0.0251,  0.0209,  0.0239],\n",
      "          [-0.0807,  0.0637,  0.0367, -0.0344, -0.0159]],\n",
      "\n",
      "         [[-0.0132, -0.0470, -0.0320, -0.0109,  0.0712],\n",
      "          [-0.0443,  0.0631, -0.0017, -0.0491,  0.0281],\n",
      "          [-0.0613, -0.0088,  0.0023,  0.0355,  0.0243],\n",
      "          [ 0.0296, -0.0796, -0.0242, -0.0591, -0.0246],\n",
      "          [ 0.0023, -0.0307, -0.0486, -0.0155, -0.0545]],\n",
      "\n",
      "         [[ 0.0459, -0.0765, -0.0045,  0.0528, -0.0658],\n",
      "          [-0.0104, -0.0130, -0.0248, -0.0262, -0.0243],\n",
      "          [-0.0515, -0.0808, -0.0661,  0.0494, -0.0627],\n",
      "          [ 0.0354,  0.0203,  0.0148, -0.0102,  0.0787],\n",
      "          [-0.0246, -0.0421, -0.0385, -0.0378, -0.0395]],\n",
      "\n",
      "         [[ 0.0011,  0.0656,  0.0066,  0.0237, -0.0022],\n",
      "          [-0.0586, -0.0762, -0.0287,  0.0471, -0.0775],\n",
      "          [ 0.0306,  0.0530, -0.0468, -0.0715, -0.0647],\n",
      "          [ 0.0270, -0.0322,  0.0119,  0.0168, -0.0303],\n",
      "          [ 0.0002, -0.0047,  0.0784, -0.0101,  0.0780]],\n",
      "\n",
      "         [[ 0.0657,  0.0159, -0.0117, -0.0725, -0.0753],\n",
      "          [-0.0647, -0.0709,  0.0305,  0.0002, -0.0104],\n",
      "          [ 0.0098, -0.0039,  0.0097,  0.0028,  0.0651],\n",
      "          [ 0.0732, -0.0411,  0.0619,  0.0427,  0.0620],\n",
      "          [ 0.0504,  0.0451,  0.0587, -0.0458, -0.0798]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0559,  0.0492,  0.0388,  0.0542, -0.0814],\n",
      "          [-0.0365,  0.0401, -0.0658, -0.0810,  0.0353],\n",
      "          [ 0.0737, -0.0081,  0.0763,  0.0561,  0.0160],\n",
      "          [-0.0181,  0.0188,  0.0629, -0.0259,  0.0242],\n",
      "          [-0.0256,  0.0562,  0.0720,  0.0106, -0.0373]],\n",
      "\n",
      "         [[ 0.0516, -0.0422, -0.0023, -0.0166,  0.0673],\n",
      "          [ 0.0013, -0.0526, -0.0749,  0.0095,  0.0490],\n",
      "          [ 0.0230,  0.0552, -0.0571,  0.0723, -0.0814],\n",
      "          [-0.0609, -0.0071,  0.0410,  0.0694,  0.0672],\n",
      "          [-0.0318, -0.0409,  0.0347, -0.0686,  0.0765]],\n",
      "\n",
      "         [[ 0.0672, -0.0291, -0.0279,  0.0005,  0.0339],\n",
      "          [ 0.0677,  0.0636,  0.0213,  0.0553, -0.0710],\n",
      "          [ 0.0007, -0.0744, -0.0631, -0.0777,  0.0183],\n",
      "          [ 0.0068, -0.0553,  0.0073,  0.0252, -0.0673],\n",
      "          [ 0.0710, -0.0498, -0.0651,  0.0157,  0.0180]],\n",
      "\n",
      "         [[ 0.0773,  0.0151, -0.0743,  0.0483, -0.0764],\n",
      "          [ 0.0725, -0.0793,  0.0067,  0.0451, -0.0092],\n",
      "          [ 0.0275,  0.0100, -0.0205,  0.0734, -0.0546],\n",
      "          [ 0.0158, -0.0079, -0.0035, -0.0574,  0.0225],\n",
      "          [-0.0748,  0.0701,  0.0767,  0.0233,  0.0182]],\n",
      "\n",
      "         [[ 0.0201, -0.0202, -0.0810,  0.0795, -0.0413],\n",
      "          [ 0.0417,  0.0723,  0.0219, -0.0803,  0.0016],\n",
      "          [-0.0394,  0.0135, -0.0815, -0.0241,  0.0624],\n",
      "          [ 0.0188, -0.0383,  0.0652, -0.0303, -0.0519],\n",
      "          [ 0.0257,  0.0468,  0.0176, -0.0253, -0.0664]],\n",
      "\n",
      "         [[ 0.0560,  0.0654,  0.0417,  0.0094, -0.0723],\n",
      "          [ 0.0185,  0.0606, -0.0114,  0.0620,  0.0195],\n",
      "          [-0.0335, -0.0420, -0.0812,  0.0057,  0.0336],\n",
      "          [ 0.0392,  0.0545, -0.0300,  0.0176, -0.0542],\n",
      "          [ 0.0534,  0.0194,  0.0053, -0.0077, -0.0411]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0739, -0.0597,  0.0324, -0.0503,  0.0103],\n",
      "          [ 0.0146, -0.0641,  0.0324, -0.0616, -0.0800],\n",
      "          [-0.0460, -0.0610,  0.0163,  0.0245, -0.0757],\n",
      "          [-0.0457,  0.0258,  0.0579,  0.0705,  0.0168],\n",
      "          [-0.0320, -0.0027, -0.0602, -0.0130, -0.0774]],\n",
      "\n",
      "         [[-0.0560,  0.0280,  0.0215, -0.0381,  0.0589],\n",
      "          [-0.0312, -0.0798,  0.0631, -0.0466,  0.0094],\n",
      "          [ 0.0094, -0.0468,  0.0105, -0.0804, -0.0553],\n",
      "          [ 0.0139,  0.0591,  0.0766,  0.0014, -0.0195],\n",
      "          [-0.0246, -0.0200,  0.0153,  0.0370, -0.0800]],\n",
      "\n",
      "         [[ 0.0534, -0.0152, -0.0584, -0.0494, -0.0177],\n",
      "          [-0.0107, -0.0525, -0.0580, -0.0676,  0.0162],\n",
      "          [ 0.0353, -0.0589,  0.0608, -0.0002, -0.0350],\n",
      "          [-0.0743, -0.0578, -0.0307, -0.0712, -0.0542],\n",
      "          [ 0.0234, -0.0624,  0.0503, -0.0715,  0.0622]],\n",
      "\n",
      "         [[-0.0389, -0.0660,  0.0736,  0.0207,  0.0794],\n",
      "          [ 0.0559, -0.0656, -0.0191,  0.0537,  0.0393],\n",
      "          [-0.0276, -0.0658,  0.0182, -0.0128, -0.0110],\n",
      "          [ 0.0562, -0.0031, -0.0124,  0.0015, -0.0351],\n",
      "          [-0.0156, -0.0370, -0.0549,  0.0583, -0.0428]],\n",
      "\n",
      "         [[ 0.0486,  0.0647, -0.0271,  0.0373, -0.0008],\n",
      "          [-0.0232,  0.0253, -0.0492,  0.0100,  0.0092],\n",
      "          [-0.0642, -0.0553, -0.0727,  0.0077,  0.0599],\n",
      "          [ 0.0290,  0.0393, -0.0497,  0.0324, -0.0465],\n",
      "          [ 0.0403,  0.0674, -0.0202,  0.0417, -0.0778]],\n",
      "\n",
      "         [[ 0.0748, -0.0753,  0.0221, -0.0428, -0.0701],\n",
      "          [-0.0751, -0.0249, -0.0312,  0.0116,  0.0385],\n",
      "          [ 0.0680,  0.0581,  0.0261, -0.0680, -0.0372],\n",
      "          [-0.0569,  0.0429,  0.0015,  0.0672, -0.0064],\n",
      "          [ 0.0148, -0.0391, -0.0159,  0.0684, -0.0157]]],\n",
      "\n",
      "\n",
      "        [[[-0.0698,  0.0498,  0.0617, -0.0567, -0.0224],\n",
      "          [ 0.0683, -0.0263, -0.0275, -0.0735, -0.0638],\n",
      "          [-0.0020,  0.0425,  0.0723, -0.0717,  0.0280],\n",
      "          [-0.0694, -0.0267,  0.0601,  0.0559,  0.0180],\n",
      "          [-0.0650,  0.0703, -0.0290,  0.0781, -0.0421]],\n",
      "\n",
      "         [[-0.0757,  0.0235, -0.0789, -0.0036, -0.0312],\n",
      "          [-0.0778,  0.0093,  0.0471, -0.0294,  0.0573],\n",
      "          [-0.0747, -0.0321,  0.0113,  0.0495,  0.0085],\n",
      "          [ 0.0255,  0.0034,  0.0144, -0.0470, -0.0412],\n",
      "          [-0.0364, -0.0755, -0.0489,  0.0579, -0.0802]],\n",
      "\n",
      "         [[-0.0126,  0.0376,  0.0737, -0.0560, -0.0185],\n",
      "          [-0.0495,  0.0070,  0.0509,  0.0508, -0.0756],\n",
      "          [ 0.0002,  0.0671,  0.0800, -0.0073, -0.0522],\n",
      "          [-0.0398,  0.0015,  0.0715,  0.0803,  0.0710],\n",
      "          [ 0.0561, -0.0080, -0.0184, -0.0517,  0.0724]],\n",
      "\n",
      "         [[ 0.0208, -0.0199,  0.0538, -0.0791,  0.0495],\n",
      "          [ 0.0435,  0.0508,  0.0464, -0.0514, -0.0008],\n",
      "          [ 0.0764, -0.0591, -0.0398,  0.0304,  0.0228],\n",
      "          [ 0.0394,  0.0475, -0.0498,  0.0499,  0.0016],\n",
      "          [ 0.0639, -0.0362,  0.0791,  0.0512,  0.0384]],\n",
      "\n",
      "         [[ 0.0808, -0.0379,  0.0736, -0.0789,  0.0295],\n",
      "          [ 0.0348, -0.0021,  0.0443,  0.0224, -0.0783],\n",
      "          [ 0.0468,  0.0560,  0.0079,  0.0634, -0.0435],\n",
      "          [ 0.0232,  0.0183, -0.0477, -0.0220,  0.0160],\n",
      "          [ 0.0789,  0.0415,  0.0302, -0.0185, -0.0110]],\n",
      "\n",
      "         [[-0.0030,  0.0791,  0.0655,  0.0116, -0.0799],\n",
      "          [-0.0360, -0.0101, -0.0563,  0.0666,  0.0093],\n",
      "          [ 0.0684, -0.0494,  0.0016,  0.0764, -0.0245],\n",
      "          [-0.0404, -0.0220, -0.0585, -0.0017,  0.0018],\n",
      "          [-0.0710, -0.0269,  0.0673, -0.0553,  0.0636]]],\n",
      "\n",
      "\n",
      "        [[[-0.0189, -0.0397, -0.0537,  0.0572, -0.0166],\n",
      "          [-0.0627, -0.0212,  0.0554,  0.0048, -0.0260],\n",
      "          [ 0.0794, -0.0020,  0.0640, -0.0155, -0.0757],\n",
      "          [-0.0091, -0.0801, -0.0686, -0.0642,  0.0196],\n",
      "          [-0.0401, -0.0456, -0.0477,  0.0311,  0.0735]],\n",
      "\n",
      "         [[ 0.0164, -0.0159, -0.0684, -0.0661, -0.0529],\n",
      "          [ 0.0702,  0.0050,  0.0620, -0.0067, -0.0797],\n",
      "          [-0.0215, -0.0789, -0.0111,  0.0056,  0.0418],\n",
      "          [-0.0770,  0.0384,  0.0653,  0.0071,  0.0507],\n",
      "          [ 0.0579, -0.0558, -0.0258,  0.0396, -0.0060]],\n",
      "\n",
      "         [[ 0.0033,  0.0067,  0.0393, -0.0316,  0.0799],\n",
      "          [-0.0205,  0.0060, -0.0222, -0.0310, -0.0629],\n",
      "          [ 0.0343,  0.0277,  0.0024,  0.0407,  0.0445],\n",
      "          [ 0.0170,  0.0093, -0.0446,  0.0575, -0.0074],\n",
      "          [-0.0554, -0.0528, -0.0224, -0.0017, -0.0812]],\n",
      "\n",
      "         [[-0.0561,  0.0294, -0.0583, -0.0631,  0.0549],\n",
      "          [ 0.0622, -0.0442,  0.0435,  0.0288, -0.0514],\n",
      "          [-0.0096, -0.0090,  0.0471,  0.0023, -0.0467],\n",
      "          [-0.0385,  0.0377, -0.0137,  0.0197,  0.0200],\n",
      "          [-0.0467, -0.0666,  0.0683, -0.0691, -0.0816]],\n",
      "\n",
      "         [[-0.0580,  0.0210,  0.0562,  0.0806, -0.0633],\n",
      "          [-0.0254,  0.0311, -0.0040,  0.0196, -0.0769],\n",
      "          [ 0.0722, -0.0307,  0.0239,  0.0779, -0.0806],\n",
      "          [ 0.0761, -0.0642,  0.0608,  0.0753,  0.0139],\n",
      "          [ 0.0395,  0.0764,  0.0288,  0.0062,  0.0695]],\n",
      "\n",
      "         [[ 0.0336,  0.0123, -0.0197,  0.0436, -0.0173],\n",
      "          [ 0.0475, -0.0394, -0.0055, -0.0014, -0.0372],\n",
      "          [ 0.0028, -0.0401,  0.0359,  0.0792,  0.0448],\n",
      "          [ 0.0387, -0.0750, -0.0501, -0.0058, -0.0208],\n",
      "          [ 0.0697, -0.0622,  0.0607,  0.0653,  0.0745]]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "**************************************************\n",
      "OrderedDict([(57, <torch.nn.utils.prune.LnStructured object at 0x0000026F82968F40>)])\n"
     ]
    }
   ],
   "source": [
    "# 第一个参数: module2, 代表要进行剪枝的特定模块, 这里指的是module2=model.conv2,\n",
    "#             说明这里要对第一个卷积层执行剪枝.\n",
    "# 第二个参数: name, 代表要对选中的模块中的哪些参数执行剪枝.\n",
    "#             这里设定为name=\"weight\", 说明是对网络中的weight剪枝, 而不对bias剪枝.\n",
    "# 第三个参数: amount, 代表要对模型中特定比例或绝对数量的参数执行剪枝.\n",
    "#             amount是一个介于0.0-1.0的float数值,代表比例, 或者一个正整数，代表指定剪裁掉多少个参数.\n",
    "# 第四个参数: n, 代表范数类型，这里n=2代表是L2范数.\n",
    "# 第五个参数: dim, 代表要进行剪枝通道(channel)的维度索引.\n",
    "\n",
    "prune.ln_structured(module2, name=\"weight\", amount=3, n=2, dim=0)\n",
    "\n",
    "# 再次打印模型参数\n",
    "print(model.state_dict().keys())\n",
    "print('*'*50)\n",
    "print(list(module2.named_parameters()))\n",
    "print('*'*50)\n",
    "print(list(module2.named_buffers()))\n",
    "print('*'*50)\n",
    "print(module2.weight)\n",
    "print('*'*50)\n",
    "print(module2._forward_pre_hooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结论: 在module的不同参数集合上应用不同的剪枝策略, 我们发现模型参数中不仅仅有了weight_orig, 也有了bias_orig. 在起到掩码张量作用的named_buffers中, 也同时出现了weight_mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "odict_keys(['conv1.bias', 'conv1.weight', 'conv2.bias', 'conv2.weight_orig', 'conv2.weight_mask', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])\n",
      "[('bias', Parameter containing:\n",
      "tensor([ 0.1717, -0.1737, -0.0202,  0.1787,  0.0203, -0.1731],\n",
      "       requires_grad=True)), ('weight', Parameter containing:\n",
      "tensor([[[[ 0.0199, -0.1327, -0.1888,  0.0135, -0.1062],\n",
      "          [-0.1044, -0.1658,  0.1823, -0.0305, -0.1942],\n",
      "          [-0.1320,  0.0258, -0.0003, -0.0305, -0.1435],\n",
      "          [-0.0992, -0.0019,  0.1422, -0.1876, -0.1113],\n",
      "          [ 0.0927, -0.0115,  0.1514,  0.0101,  0.1839]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1230, -0.1329,  0.0667,  0.0866, -0.1326],\n",
      "          [ 0.1693,  0.0348,  0.0219,  0.0047, -0.0020],\n",
      "          [ 0.1351, -0.0226,  0.1412, -0.0835,  0.1451],\n",
      "          [-0.1025, -0.0730, -0.0170, -0.0361, -0.0915],\n",
      "          [ 0.0103,  0.0657, -0.0003, -0.1348,  0.1867]]],\n",
      "\n",
      "\n",
      "        [[[-0.0462, -0.1972,  0.1027,  0.0306,  0.0201],\n",
      "          [ 0.1154,  0.1172, -0.0053,  0.1465,  0.1623],\n",
      "          [-0.0238, -0.0572,  0.1679,  0.0517, -0.0271],\n",
      "          [-0.0193, -0.0926, -0.1101,  0.1252,  0.1987],\n",
      "          [-0.1187,  0.0380, -0.1373,  0.1509, -0.1453]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000, -0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000, -0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0564, -0.1212, -0.0545,  0.0416,  0.1363],\n",
      "          [-0.1235,  0.1163, -0.1436,  0.0375,  0.1218],\n",
      "          [-0.0306, -0.1670, -0.0091, -0.0653, -0.1857],\n",
      "          [ 0.0084,  0.0360,  0.0639, -0.0800,  0.0445],\n",
      "          [ 0.1117,  0.1721, -0.1586, -0.1977, -0.1695]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0169,  0.1757, -0.0178, -0.0448,  0.1480],\n",
      "          [-0.0359, -0.1438, -0.0888, -0.0089, -0.1515],\n",
      "          [ 0.1817, -0.0435, -0.0259,  0.0408, -0.0096],\n",
      "          [-0.1641,  0.0599,  0.0107,  0.0912,  0.1782],\n",
      "          [ 0.1733,  0.1339,  0.0162, -0.0256, -0.0540]]]], requires_grad=True))]\n",
      "**************************************************\n",
      "[]\n",
      "**************************************************\n",
      "OrderedDict()\n"
     ]
    }
   ],
   "source": [
    "# 对module1执行剪枝永久化操作remove\n",
    "prune.remove(module1, 'weight')\n",
    "print('*'*50)\n",
    "\n",
    "# 将剪枝后的模型的状态字典打印出来\n",
    "print(model.state_dict().keys())\n",
    "\n",
    "# 再次打印模型参数\n",
    "print(list(module1.named_parameters()))\n",
    "print('*'*50)\n",
    "\n",
    "# r再次打印模型mask buffers参数\n",
    "print(list(module1.named_buffers()))\n",
    "print('*'*50)\n",
    "\n",
    "# 再次打印模型的_forward_pre_hooks\n",
    "print(module1._forward_pre_hooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "odict_keys(['conv1.bias', 'conv1.weight', 'conv2.bias', 'conv2.weight', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])\n",
      "[('bias', Parameter containing:\n",
      "tensor([-0.0397, -0.0521, -0.0037, -0.0105,  0.0705, -0.0071,  0.0337,  0.0399,\n",
      "         0.0788, -0.0610,  0.0180,  0.0376, -0.0044,  0.0465,  0.0462, -0.0157],\n",
      "       requires_grad=True)), ('weight', Parameter containing:\n",
      "tensor([[[[ 0.0210, -0.0655,  0.0230,  0.0613, -0.0271],\n",
      "          [-0.0699,  0.0637, -0.0151, -0.0677,  0.0258],\n",
      "          [ 0.0524,  0.0122,  0.0715,  0.0532,  0.0003],\n",
      "          [-0.0535,  0.0701,  0.0092, -0.0380, -0.0733],\n",
      "          [-0.0700,  0.0787, -0.0032,  0.0699,  0.0131]],\n",
      "\n",
      "         [[-0.0752, -0.0405, -0.0301, -0.0688,  0.0344],\n",
      "          [ 0.0741, -0.0329, -0.0034, -0.0423,  0.0569],\n",
      "          [-0.0753,  0.0278,  0.0411,  0.0265,  0.0094],\n",
      "          [-0.0207,  0.0377, -0.0015,  0.0758, -0.0226],\n",
      "          [-0.0267, -0.0606, -0.0489,  0.0108,  0.0403]],\n",
      "\n",
      "         [[ 0.0536,  0.0718,  0.0239,  0.0475,  0.0764],\n",
      "          [ 0.0763, -0.0240, -0.0424, -0.0742, -0.0398],\n",
      "          [-0.0459, -0.0795,  0.0269,  0.0177, -0.0796],\n",
      "          [ 0.0132,  0.0038,  0.0613, -0.0618,  0.0039],\n",
      "          [-0.0528,  0.0310, -0.0150,  0.0130,  0.0704]],\n",
      "\n",
      "         [[ 0.0786,  0.0597,  0.0314, -0.0299, -0.0053],\n",
      "          [ 0.0206, -0.0619,  0.0023, -0.0568, -0.0475],\n",
      "          [-0.0202,  0.0103,  0.0073,  0.0738, -0.0806],\n",
      "          [ 0.0473,  0.0063, -0.0282,  0.0774,  0.0347],\n",
      "          [ 0.0740, -0.0478, -0.0812,  0.0178, -0.0201]],\n",
      "\n",
      "         [[ 0.0555, -0.0467,  0.0303, -0.0006,  0.0757],\n",
      "          [ 0.0531,  0.0612, -0.0816,  0.0536,  0.0020],\n",
      "          [-0.0483,  0.0280,  0.0397, -0.0647, -0.0102],\n",
      "          [-0.0343, -0.0011,  0.0508,  0.0690,  0.0580],\n",
      "          [-0.0516,  0.0205, -0.0308,  0.0098,  0.0451]],\n",
      "\n",
      "         [[ 0.0092, -0.0552,  0.0511,  0.0166,  0.0659],\n",
      "          [-0.0155, -0.0232, -0.0073, -0.0257,  0.0024],\n",
      "          [-0.0359, -0.0160, -0.0342,  0.0576, -0.0215],\n",
      "          [-0.0741,  0.0204,  0.0146,  0.0103, -0.0683],\n",
      "          [-0.0471,  0.0516, -0.0497, -0.0337, -0.0073]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0652, -0.0267, -0.0183, -0.0653, -0.0779],\n",
      "          [-0.0100, -0.0438, -0.0756,  0.0215,  0.0148],\n",
      "          [-0.0014, -0.0703,  0.0468, -0.0521, -0.0122],\n",
      "          [ 0.0435, -0.0666,  0.0672,  0.0728, -0.0143],\n",
      "          [ 0.0723,  0.0352,  0.0044, -0.0535,  0.0267]],\n",
      "\n",
      "         [[-0.0402,  0.0512,  0.0296, -0.0010,  0.0147],\n",
      "          [-0.0795, -0.0532,  0.0080,  0.0340,  0.0756],\n",
      "          [-0.0013, -0.0749, -0.0349, -0.0768, -0.0809],\n",
      "          [ 0.0172,  0.0747,  0.0251,  0.0209,  0.0239],\n",
      "          [-0.0807,  0.0637,  0.0367, -0.0344, -0.0159]],\n",
      "\n",
      "         [[-0.0132, -0.0470, -0.0320, -0.0109,  0.0712],\n",
      "          [-0.0443,  0.0631, -0.0017, -0.0491,  0.0281],\n",
      "          [-0.0613, -0.0088,  0.0023,  0.0355,  0.0243],\n",
      "          [ 0.0296, -0.0796, -0.0242, -0.0591, -0.0246],\n",
      "          [ 0.0023, -0.0307, -0.0486, -0.0155, -0.0545]],\n",
      "\n",
      "         [[ 0.0459, -0.0765, -0.0045,  0.0528, -0.0658],\n",
      "          [-0.0104, -0.0130, -0.0248, -0.0262, -0.0243],\n",
      "          [-0.0515, -0.0808, -0.0661,  0.0494, -0.0627],\n",
      "          [ 0.0354,  0.0203,  0.0148, -0.0102,  0.0787],\n",
      "          [-0.0246, -0.0421, -0.0385, -0.0378, -0.0395]],\n",
      "\n",
      "         [[ 0.0011,  0.0656,  0.0066,  0.0237, -0.0022],\n",
      "          [-0.0586, -0.0762, -0.0287,  0.0471, -0.0775],\n",
      "          [ 0.0306,  0.0530, -0.0468, -0.0715, -0.0647],\n",
      "          [ 0.0270, -0.0322,  0.0119,  0.0168, -0.0303],\n",
      "          [ 0.0002, -0.0047,  0.0784, -0.0101,  0.0780]],\n",
      "\n",
      "         [[ 0.0657,  0.0159, -0.0117, -0.0725, -0.0753],\n",
      "          [-0.0647, -0.0709,  0.0305,  0.0002, -0.0104],\n",
      "          [ 0.0098, -0.0039,  0.0097,  0.0028,  0.0651],\n",
      "          [ 0.0732, -0.0411,  0.0619,  0.0427,  0.0620],\n",
      "          [ 0.0504,  0.0451,  0.0587, -0.0458, -0.0798]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0559,  0.0492,  0.0388,  0.0542, -0.0814],\n",
      "          [-0.0365,  0.0401, -0.0658, -0.0810,  0.0353],\n",
      "          [ 0.0737, -0.0081,  0.0763,  0.0561,  0.0160],\n",
      "          [-0.0181,  0.0188,  0.0629, -0.0259,  0.0242],\n",
      "          [-0.0256,  0.0562,  0.0720,  0.0106, -0.0373]],\n",
      "\n",
      "         [[ 0.0516, -0.0422, -0.0023, -0.0166,  0.0673],\n",
      "          [ 0.0013, -0.0526, -0.0749,  0.0095,  0.0490],\n",
      "          [ 0.0230,  0.0552, -0.0571,  0.0723, -0.0814],\n",
      "          [-0.0609, -0.0071,  0.0410,  0.0694,  0.0672],\n",
      "          [-0.0318, -0.0409,  0.0347, -0.0686,  0.0765]],\n",
      "\n",
      "         [[ 0.0672, -0.0291, -0.0279,  0.0005,  0.0339],\n",
      "          [ 0.0677,  0.0636,  0.0213,  0.0553, -0.0710],\n",
      "          [ 0.0007, -0.0744, -0.0631, -0.0777,  0.0183],\n",
      "          [ 0.0068, -0.0553,  0.0073,  0.0252, -0.0673],\n",
      "          [ 0.0710, -0.0498, -0.0651,  0.0157,  0.0180]],\n",
      "\n",
      "         [[ 0.0773,  0.0151, -0.0743,  0.0483, -0.0764],\n",
      "          [ 0.0725, -0.0793,  0.0067,  0.0451, -0.0092],\n",
      "          [ 0.0275,  0.0100, -0.0205,  0.0734, -0.0546],\n",
      "          [ 0.0158, -0.0079, -0.0035, -0.0574,  0.0225],\n",
      "          [-0.0748,  0.0701,  0.0767,  0.0233,  0.0182]],\n",
      "\n",
      "         [[ 0.0201, -0.0202, -0.0810,  0.0795, -0.0413],\n",
      "          [ 0.0417,  0.0723,  0.0219, -0.0803,  0.0016],\n",
      "          [-0.0394,  0.0135, -0.0815, -0.0241,  0.0624],\n",
      "          [ 0.0188, -0.0383,  0.0652, -0.0303, -0.0519],\n",
      "          [ 0.0257,  0.0468,  0.0176, -0.0253, -0.0664]],\n",
      "\n",
      "         [[ 0.0560,  0.0654,  0.0417,  0.0094, -0.0723],\n",
      "          [ 0.0185,  0.0606, -0.0114,  0.0620,  0.0195],\n",
      "          [-0.0335, -0.0420, -0.0812,  0.0057,  0.0336],\n",
      "          [ 0.0392,  0.0545, -0.0300,  0.0176, -0.0542],\n",
      "          [ 0.0534,  0.0194,  0.0053, -0.0077, -0.0411]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0739, -0.0597,  0.0324, -0.0503,  0.0103],\n",
      "          [ 0.0146, -0.0641,  0.0324, -0.0616, -0.0800],\n",
      "          [-0.0460, -0.0610,  0.0163,  0.0245, -0.0757],\n",
      "          [-0.0457,  0.0258,  0.0579,  0.0705,  0.0168],\n",
      "          [-0.0320, -0.0027, -0.0602, -0.0130, -0.0774]],\n",
      "\n",
      "         [[-0.0560,  0.0280,  0.0215, -0.0381,  0.0589],\n",
      "          [-0.0312, -0.0798,  0.0631, -0.0466,  0.0094],\n",
      "          [ 0.0094, -0.0468,  0.0105, -0.0804, -0.0553],\n",
      "          [ 0.0139,  0.0591,  0.0766,  0.0014, -0.0195],\n",
      "          [-0.0246, -0.0200,  0.0153,  0.0370, -0.0800]],\n",
      "\n",
      "         [[ 0.0534, -0.0152, -0.0584, -0.0494, -0.0177],\n",
      "          [-0.0107, -0.0525, -0.0580, -0.0676,  0.0162],\n",
      "          [ 0.0353, -0.0589,  0.0608, -0.0002, -0.0350],\n",
      "          [-0.0743, -0.0578, -0.0307, -0.0712, -0.0542],\n",
      "          [ 0.0234, -0.0624,  0.0503, -0.0715,  0.0622]],\n",
      "\n",
      "         [[-0.0389, -0.0660,  0.0736,  0.0207,  0.0794],\n",
      "          [ 0.0559, -0.0656, -0.0191,  0.0537,  0.0393],\n",
      "          [-0.0276, -0.0658,  0.0182, -0.0128, -0.0110],\n",
      "          [ 0.0562, -0.0031, -0.0124,  0.0015, -0.0351],\n",
      "          [-0.0156, -0.0370, -0.0549,  0.0583, -0.0428]],\n",
      "\n",
      "         [[ 0.0486,  0.0647, -0.0271,  0.0373, -0.0008],\n",
      "          [-0.0232,  0.0253, -0.0492,  0.0100,  0.0092],\n",
      "          [-0.0642, -0.0553, -0.0727,  0.0077,  0.0599],\n",
      "          [ 0.0290,  0.0393, -0.0497,  0.0324, -0.0465],\n",
      "          [ 0.0403,  0.0674, -0.0202,  0.0417, -0.0778]],\n",
      "\n",
      "         [[ 0.0748, -0.0753,  0.0221, -0.0428, -0.0701],\n",
      "          [-0.0751, -0.0249, -0.0312,  0.0116,  0.0385],\n",
      "          [ 0.0680,  0.0581,  0.0261, -0.0680, -0.0372],\n",
      "          [-0.0569,  0.0429,  0.0015,  0.0672, -0.0064],\n",
      "          [ 0.0148, -0.0391, -0.0159,  0.0684, -0.0157]]],\n",
      "\n",
      "\n",
      "        [[[-0.0698,  0.0498,  0.0617, -0.0567, -0.0224],\n",
      "          [ 0.0683, -0.0263, -0.0275, -0.0735, -0.0638],\n",
      "          [-0.0020,  0.0425,  0.0723, -0.0717,  0.0280],\n",
      "          [-0.0694, -0.0267,  0.0601,  0.0559,  0.0180],\n",
      "          [-0.0650,  0.0703, -0.0290,  0.0781, -0.0421]],\n",
      "\n",
      "         [[-0.0757,  0.0235, -0.0789, -0.0036, -0.0312],\n",
      "          [-0.0778,  0.0093,  0.0471, -0.0294,  0.0573],\n",
      "          [-0.0747, -0.0321,  0.0113,  0.0495,  0.0085],\n",
      "          [ 0.0255,  0.0034,  0.0144, -0.0470, -0.0412],\n",
      "          [-0.0364, -0.0755, -0.0489,  0.0579, -0.0802]],\n",
      "\n",
      "         [[-0.0126,  0.0376,  0.0737, -0.0560, -0.0185],\n",
      "          [-0.0495,  0.0070,  0.0509,  0.0508, -0.0756],\n",
      "          [ 0.0002,  0.0671,  0.0800, -0.0073, -0.0522],\n",
      "          [-0.0398,  0.0015,  0.0715,  0.0803,  0.0710],\n",
      "          [ 0.0561, -0.0080, -0.0184, -0.0517,  0.0724]],\n",
      "\n",
      "         [[ 0.0208, -0.0199,  0.0538, -0.0791,  0.0495],\n",
      "          [ 0.0435,  0.0508,  0.0464, -0.0514, -0.0008],\n",
      "          [ 0.0764, -0.0591, -0.0398,  0.0304,  0.0228],\n",
      "          [ 0.0394,  0.0475, -0.0498,  0.0499,  0.0016],\n",
      "          [ 0.0639, -0.0362,  0.0791,  0.0512,  0.0384]],\n",
      "\n",
      "         [[ 0.0808, -0.0379,  0.0736, -0.0789,  0.0295],\n",
      "          [ 0.0348, -0.0021,  0.0443,  0.0224, -0.0783],\n",
      "          [ 0.0468,  0.0560,  0.0079,  0.0634, -0.0435],\n",
      "          [ 0.0232,  0.0183, -0.0477, -0.0220,  0.0160],\n",
      "          [ 0.0789,  0.0415,  0.0302, -0.0185, -0.0110]],\n",
      "\n",
      "         [[-0.0030,  0.0791,  0.0655,  0.0116, -0.0799],\n",
      "          [-0.0360, -0.0101, -0.0563,  0.0666,  0.0093],\n",
      "          [ 0.0684, -0.0494,  0.0016,  0.0764, -0.0245],\n",
      "          [-0.0404, -0.0220, -0.0585, -0.0017,  0.0018],\n",
      "          [-0.0710, -0.0269,  0.0673, -0.0553,  0.0636]]],\n",
      "\n",
      "\n",
      "        [[[-0.0189, -0.0397, -0.0537,  0.0572, -0.0166],\n",
      "          [-0.0627, -0.0212,  0.0554,  0.0048, -0.0260],\n",
      "          [ 0.0794, -0.0020,  0.0640, -0.0155, -0.0757],\n",
      "          [-0.0091, -0.0801, -0.0686, -0.0642,  0.0196],\n",
      "          [-0.0401, -0.0456, -0.0477,  0.0311,  0.0735]],\n",
      "\n",
      "         [[ 0.0164, -0.0159, -0.0684, -0.0661, -0.0529],\n",
      "          [ 0.0702,  0.0050,  0.0620, -0.0067, -0.0797],\n",
      "          [-0.0215, -0.0789, -0.0111,  0.0056,  0.0418],\n",
      "          [-0.0770,  0.0384,  0.0653,  0.0071,  0.0507],\n",
      "          [ 0.0579, -0.0558, -0.0258,  0.0396, -0.0060]],\n",
      "\n",
      "         [[ 0.0033,  0.0067,  0.0393, -0.0316,  0.0799],\n",
      "          [-0.0205,  0.0060, -0.0222, -0.0310, -0.0629],\n",
      "          [ 0.0343,  0.0277,  0.0024,  0.0407,  0.0445],\n",
      "          [ 0.0170,  0.0093, -0.0446,  0.0575, -0.0074],\n",
      "          [-0.0554, -0.0528, -0.0224, -0.0017, -0.0812]],\n",
      "\n",
      "         [[-0.0561,  0.0294, -0.0583, -0.0631,  0.0549],\n",
      "          [ 0.0622, -0.0442,  0.0435,  0.0288, -0.0514],\n",
      "          [-0.0096, -0.0090,  0.0471,  0.0023, -0.0467],\n",
      "          [-0.0385,  0.0377, -0.0137,  0.0197,  0.0200],\n",
      "          [-0.0467, -0.0666,  0.0683, -0.0691, -0.0816]],\n",
      "\n",
      "         [[-0.0580,  0.0210,  0.0562,  0.0806, -0.0633],\n",
      "          [-0.0254,  0.0311, -0.0040,  0.0196, -0.0769],\n",
      "          [ 0.0722, -0.0307,  0.0239,  0.0779, -0.0806],\n",
      "          [ 0.0761, -0.0642,  0.0608,  0.0753,  0.0139],\n",
      "          [ 0.0395,  0.0764,  0.0288,  0.0062,  0.0695]],\n",
      "\n",
      "         [[ 0.0336,  0.0123, -0.0197,  0.0436, -0.0173],\n",
      "          [ 0.0475, -0.0394, -0.0055, -0.0014, -0.0372],\n",
      "          [ 0.0028, -0.0401,  0.0359,  0.0792,  0.0448],\n",
      "          [ 0.0387, -0.0750, -0.0501, -0.0058, -0.0208],\n",
      "          [ 0.0697, -0.0622,  0.0607,  0.0653,  0.0745]]]], requires_grad=True))]\n",
      "**************************************************\n",
      "[]\n",
      "**************************************************\n",
      "OrderedDict()\n"
     ]
    }
   ],
   "source": [
    "# 对module2执行剪枝永久化操作remove\n",
    "prune.remove(module2, 'weight')\n",
    "print('*'*50)\n",
    "\n",
    "# 将剪枝后的模型的状态字典打印出来\n",
    "print(model.state_dict().keys())\n",
    "\n",
    "# 再次打印模型参数\n",
    "print(list(module2.named_parameters()))\n",
    "print('*'*50)\n",
    "\n",
    "# r再次打印模型mask buffers参数\n",
    "print(list(module2.named_buffers()))\n",
    "print('*'*50)\n",
    "\n",
    "# 再次打印模型的_forward_pre_hooks\n",
    "print(module2._forward_pre_hooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结论: 对模型的weight执行remove操作后, 模型参数集合中只剩下bias_orig了, weight_orig消失, 变成了weight, 说明针对weight的剪枝已经永久化生效. 对于named_buffers张量打印可以看出, 只剩下bias_mask了, 因为针对weight做掩码的weight_mask已经生效完毕, 不再需要保留了. 同理, 在_forward_pre_hooks中也只剩下针对bias做剪枝的函数了."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.多参数模块的剪枝(Pruning multiple parameters).¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])\n",
      "**************************************************\n",
      "dict_keys([])\n",
      "**************************************************\n",
      "dict_keys(['conv1.weight_mask', 'conv2.weight_mask', 'fc1.weight_mask', 'fc2.weight_mask', 'fc3.weight_mask'])\n",
      "**************************************************\n",
      "odict_keys(['conv1.bias', 'conv1.weight_orig', 'conv1.weight_mask', 'conv2.bias', 'conv2.weight_orig', 'conv2.weight_mask', 'fc1.bias', 'fc1.weight_orig', 'fc1.weight_mask', 'fc2.bias', 'fc2.weight_orig', 'fc2.weight_mask', 'fc3.bias', 'fc3.weight_orig', 'fc3.weight_mask'])\n"
     ]
    }
   ],
   "source": [
    "model = LeNet().to(device=device)\n",
    "\n",
    "# 打印初始模型的所有状态字典\n",
    "print(model.state_dict().keys())\n",
    "print('*'*50)\n",
    "\n",
    "# 打印初始模型的mask buffers张量字典名称\n",
    "print(dict(model.named_buffers()).keys())\n",
    "print('*'*50)\n",
    "\n",
    "# 对于模型进行分模块参数的剪枝\n",
    "for name, module in model.named_modules():\n",
    "    # 对模型中所有的卷积层执行l1_unstructured剪枝操作, 选取20%的参数剪枝\n",
    "    if isinstance(module, torch.nn.Conv2d):\n",
    "        prune.l1_unstructured(module, name=\"weight\", amount=0.2)\n",
    "    # 对模型中所有全连接层执行ln_structured剪枝操作, 选取40%的参数剪枝\n",
    "    elif isinstance(module, torch.nn.Linear):\n",
    "        prune.ln_structured(module, name=\"weight\", amount=0.4, n=2, dim=0)\n",
    "\n",
    "# 打印多参数模块剪枝后的mask buffers张量字典名称\n",
    "print(dict(model.named_buffers()).keys())\n",
    "print('*'*50)\n",
    "\n",
    "# 打印多参数模块剪枝后模型的所有状态字典名称\n",
    "print(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.全局剪枝(GLobal pruning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更普遍也更通用的剪枝策略是采用全局剪枝(global pruning), 比如在整体网络的视角下剪枝掉20%的权重参数, 而不是在每一层上都剪枝掉20%的权重参数. 采用全局剪枝后, 不同的层被剪掉的百分比不同."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])\n",
      "**************************************************\n",
      "odict_keys(['conv1.bias', 'conv1.weight_orig', 'conv1.weight_mask', 'conv2.bias', 'conv2.weight_orig', 'conv2.weight_mask', 'fc1.bias', 'fc1.weight_orig', 'fc1.weight_mask', 'fc2.bias', 'fc2.weight_orig', 'fc2.weight_mask', 'fc3.weight', 'fc3.bias'])\n"
     ]
    }
   ],
   "source": [
    "model = LeNet().to(device=device)\n",
    "\n",
    "# 首先打印初始化模型的状态字典\n",
    "print(model.state_dict().keys())\n",
    "print('*'*50)\n",
    "\n",
    "# 构建参数集合, 决定哪些层, 哪些参数集合参与剪枝\n",
    "parameters_to_prune = (\n",
    "            (model.conv1, 'weight'),\n",
    "            (model.conv2, 'weight'),\n",
    "            (model.fc1, 'weight'),\n",
    "            (model.fc2, 'weight'))\n",
    "\n",
    "# 调用prune中的全局剪枝函数global_unstructured执行剪枝操作, 此处针对整体模型中的20%参数量进行剪枝\n",
    "prune.global_unstructured(parameters_to_prune, pruning_method=prune.L1Unstructured, amount=0.2)\n",
    "\n",
    "# 最后打印剪枝后的模型的状态字典\n",
    "print(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "针对模型剪枝后, 不同的层会有不同比例的权重参数被剪掉, 利用代码打印出来看看:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity in conv1.weight: 8.67%\n",
      "Sparsity in conv2.weight: 16.67%\n",
      "Sparsity in fc1.weight: 21.78%\n",
      "Sparsity in fc2.weight: 15.55%\n",
      "Global sparsity: 20.00%\n"
     ]
    }
   ],
   "source": [
    "model = LeNet().to(device=device)\n",
    "\n",
    "parameters_to_prune = (\n",
    "            (model.conv1, 'weight'),\n",
    "            (model.conv2, 'weight'),\n",
    "            (model.fc1, 'weight'),\n",
    "            (model.fc2, 'weight'))\n",
    "\n",
    "prune.global_unstructured(parameters_to_prune, pruning_method=prune.L1Unstructured, amount=0.2)\n",
    "\n",
    "print(\n",
    "    \"Sparsity in conv1.weight: {:.2f}%\".format(\n",
    "    100. * float(torch.sum(model.conv1.weight == 0))\n",
    "    / float(model.conv1.weight.nelement())\n",
    "    ))\n",
    "\n",
    "print(\n",
    "    \"Sparsity in conv2.weight: {:.2f}%\".format(\n",
    "    100. * float(torch.sum(model.conv2.weight == 0))\n",
    "    / float(model.conv2.weight.nelement())\n",
    "    ))\n",
    "\n",
    "print(\n",
    "    \"Sparsity in fc1.weight: {:.2f}%\".format(\n",
    "    100. * float(torch.sum(model.fc1.weight == 0))\n",
    "    / float(model.fc1.weight.nelement())\n",
    "    ))\n",
    "\n",
    "print(\n",
    "    \"Sparsity in fc2.weight: {:.2f}%\".format(\n",
    "    100. * float(torch.sum(model.fc2.weight == 0))\n",
    "    / float(model.fc2.weight.nelement())\n",
    "    ))\n",
    "\n",
    "\n",
    "print(\n",
    "    \"Global sparsity: {:.2f}%\".format(\n",
    "    100. * float(torch.sum(model.conv1.weight == 0)\n",
    "               + torch.sum(model.conv2.weight == 0)\n",
    "               + torch.sum(model.fc1.weight == 0)\n",
    "               + torch.sum(model.fc2.weight == 0))\n",
    "         / float(model.conv1.weight.nelement()\n",
    "               + model.conv2.weight.nelement()\n",
    "               + model.fc1.weight.nelement()\n",
    "               + model.fc2.weight.nelement())\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结论: 当采用全局剪枝策略的时候(假定20%比例参数参与剪枝), 仅保证模型总体参数量的20%被剪枝掉, 具体到每一层的情况则由模型的具体参数分布情况来定.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.用户自定义剪枝(Custom pruning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "剪枝模型通过继承class BasePruningMethod()来执行剪枝, 内部有若干方法: call, apply_mask, apply, prune, remove等等. 一般来说, 用户只需要实现__init__, 和compute_mask两个函数即可完成自定义的剪枝规则设定."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义剪枝方法的类, 一定要继承prune.BasePruningMethod\n",
    "class custom_prune(prune.BasePruningMethod):\n",
    "    PRUNING_TYPE = \"unstructured\"\n",
    "\n",
    "    # 内部实现compute_mask函数, 完成程序员自己定义的剪枝规则, 本质上就是如何去mask掉权重参数\n",
    "    def compute_mask(self, t, default_mask):\n",
    "        mask = default_mask.clone()\n",
    "        # 此处定义的规则是每隔一个参数就遮掩掉一个, 最终参与剪枝的参数量的50%被mask掉\n",
    "        mask.view(-1)[::2] = 0\n",
    "        return mask\n",
    "\n",
    "# 自定义剪枝方法的函数, 内部直接调用剪枝类的方法apply\n",
    "def custome_unstructured_pruning(module, name):\n",
    "    custom_prune.apply(module, name)\n",
    "    return module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.])\n",
      "1.9998550415039062 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# 实例化模型类\n",
    "model = LeNet().to(device=device)\n",
    "\n",
    "start = time.time()\n",
    "# 调用自定义剪枝方法的函数, 对model中的第2个全连接层fc2中的偏置bias执行自定义剪枝\n",
    "custome_unstructured_pruning(model.fc2, name=\"bias\")\n",
    "\n",
    "# 剪枝成功的最大标志, 就是拥有了bias_mask参数\n",
    "print(model.fc2.bias_mask)\n",
    "\n",
    "# 打印一下自定义剪枝的耗时\n",
    "duration = time.time() - start\n",
    "print(duration * 1000, 'ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结论: 打印出来的bias_mask张量, 完全是按照预定义的方式每隔一位遮掩掉一位, 0和1交替出现, 后续执行remove操作的时候, 原始的bias_orig中的权重就会同样的被每隔一位剪枝掉一位. 在GPU机器上执行自定义剪枝速度特别快, 仅需1.7ms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
