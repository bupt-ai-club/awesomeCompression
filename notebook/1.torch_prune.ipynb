{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型剪枝实践\n",
    "Pytorch在1.4.0版本开始，加入了剪枝操作，在torch.nn.utils.prune模块中，本项目按照剪枝范围划分，将其分以下几种剪枝方式:\n",
    "- 局部剪枝（Local Pruning）\n",
    "  - 结构化剪枝\n",
    "    - 随机结构化剪枝（random_structured）\n",
    "    - 范数结构化剪枝（ln_structured）\n",
    "  - 非结构化剪枝\n",
    "    - 随机非结构化剪枝（random_unstructured）\n",
    "    - 范数非结构化剪枝（l1_unstructured）\n",
    "- 全局剪枝（Global Pruning）\n",
    "  - 非结构化剪枝（global_unstructured）\n",
    "- 自定义剪枝（Custom  Pruning）\n",
    "  \n",
    "**注：** 全局剪枝只有非结构化剪枝方式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、局部剪枝\n",
    "首先介绍局部剪枝（Local Pruning）方式，指的是对网络的单个层或局部范围内进行剪枝。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 结构化剪枝\n",
    "按照剪枝方式划分，可以分为结构化剪枝和非结构化剪枝方式。非结构化剪枝会随机地将一些权重参数变为0，结构化剪枝则将某个维度某些通道变成0。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 随机结构化剪枝（random_structured）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建一个经典的LeNet网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个LeNet网络\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(in_features=16 * 4 * 4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=84)\n",
    "        self.fc3 = nn.Linear(in_features=84, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.maxpool(F.relu(self.conv1(x)))\n",
    "        x = self.maxpool(F.relu(self.conv2(x)))\n",
    "\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LeNet().to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 6, 24, 24]             156\n",
      "         MaxPool2d-2            [-1, 6, 12, 12]               0\n",
      "            Conv2d-3             [-1, 16, 8, 8]           2,416\n",
      "         MaxPool2d-4             [-1, 16, 4, 4]               0\n",
      "            Linear-5                  [-1, 120]          30,840\n",
      "            Linear-6                   [-1, 84]          10,164\n",
      "            Linear-7                   [-1, 10]             850\n",
      "================================================================\n",
      "Total params: 44,426\n",
      "Trainable params: 44,426\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.04\n",
      "Params size (MB): 0.17\n",
      "Estimated Total Size (MB): 0.22\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 打印模型结构\n",
    "summary(model, input_size=(1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('weight', Parameter containing:\n",
      "tensor([[[[ 0.1686, -0.0279,  0.0093,  0.0203,  0.1026],\n",
      "          [-0.0616, -0.0219, -0.1877, -0.1741, -0.1490],\n",
      "          [-0.0866,  0.0452, -0.1431,  0.0458, -0.1175],\n",
      "          [-0.1405,  0.1346,  0.1511, -0.1854, -0.1366],\n",
      "          [-0.1095, -0.1114, -0.1008,  0.1839, -0.0591]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1176, -0.1028,  0.1988, -0.0817, -0.0150],\n",
      "          [ 0.1538, -0.1620,  0.0233,  0.0813, -0.1602],\n",
      "          [ 0.1265, -0.0829,  0.1144,  0.1668, -0.0539],\n",
      "          [-0.1210,  0.1312, -0.0959, -0.0105,  0.1205],\n",
      "          [-0.0160,  0.1774, -0.1242, -0.1329, -0.1535]]],\n",
      "\n",
      "\n",
      "        [[[-0.1265,  0.1533, -0.0417, -0.1237,  0.0965],\n",
      "          [ 0.0884, -0.0186,  0.0079, -0.1489, -0.1696],\n",
      "          [ 0.0891, -0.0880, -0.0832,  0.1880,  0.1759],\n",
      "          [ 0.1442,  0.0126, -0.1202, -0.0170,  0.0432],\n",
      "          [-0.1886, -0.0143, -0.1024,  0.1467,  0.1232]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0314,  0.0240,  0.1634, -0.0672, -0.0532],\n",
      "          [-0.1676,  0.0303, -0.0094, -0.1490,  0.1117],\n",
      "          [ 0.0315,  0.0096, -0.1286, -0.0136, -0.0815],\n",
      "          [-0.1374, -0.1207,  0.0913,  0.0391, -0.0015],\n",
      "          [-0.0103, -0.1370,  0.0183,  0.0392,  0.0975]]],\n",
      "\n",
      "\n",
      "        [[[-0.0483,  0.0519, -0.0058,  0.0189, -0.1174],\n",
      "          [ 0.0462, -0.0780, -0.0599,  0.0112,  0.1347],\n",
      "          [ 0.0984,  0.1140, -0.0906, -0.1727,  0.0907],\n",
      "          [ 0.1641,  0.1185,  0.1957, -0.0676,  0.1687],\n",
      "          [-0.0162, -0.0926, -0.0085, -0.1415, -0.0388]]],\n",
      "\n",
      "\n",
      "        [[[-0.1036, -0.0583,  0.0769, -0.0652,  0.1137],\n",
      "          [-0.0463,  0.1175,  0.0850,  0.0853,  0.0254],\n",
      "          [-0.0856,  0.1141, -0.1028, -0.0783, -0.0793],\n",
      "          [-0.0620, -0.1171,  0.1849, -0.1871, -0.1474],\n",
      "          [ 0.1293,  0.0534,  0.0419,  0.1507,  0.1913]]]], requires_grad=True)), ('bias', Parameter containing:\n",
      "tensor([ 0.0585, -0.0013, -0.1102,  0.1541,  0.1304, -0.1803],\n",
      "       requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "# 打印第一个卷积层的参数\n",
    "module = model.conv1\n",
    "print(list(module.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# 打印module中的属性张量named_buffers，初始时为空列表\n",
    "print(list(module.named_buffers()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])\n"
     ]
    }
   ],
   "source": [
    "# 打印模型的状态字典，状态字典里包含了所有的参数\n",
    "print(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第一个参数: module, 代表要进行剪枝的特定模块, 这里指的是module=model.conv1,\n",
    "#             说明这里要对第一个卷积层执行剪枝.\n",
    "# 第二个参数: name, 代表要对选中的模块中的哪些参数执行剪枝.\n",
    "#             这里设定为name=\"weight\", 说明是对网络中的weight剪枝, 而不对bias剪枝.\n",
    "# 第三个参数: amount, 代表要对模型中特定比例或绝对数量的参数执行剪枝.\n",
    "#             amount是一个介于0.0-1.0的float数值,代表比例, 或者一个正整数，代表指定剪裁掉多少个参数.\n",
    "# 第四个参数: dim, 代表要进行剪枝通道(channel)的维度索引.\n",
    "#            \n",
    "\n",
    "prune.random_structured(module, name=\"weight\", amount=2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['conv1.bias', 'conv1.weight_orig', 'conv1.weight_mask', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])\n"
     ]
    }
   ],
   "source": [
    "# 再次打印模型的状态字典，观察conv1层\n",
    "print(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bias', Parameter containing:\n",
      "tensor([ 0.0585, -0.0013, -0.1102,  0.1541,  0.1304, -0.1803],\n",
      "       requires_grad=True)), ('weight_orig', Parameter containing:\n",
      "tensor([[[[ 0.1686, -0.0279,  0.0093,  0.0203,  0.1026],\n",
      "          [-0.0616, -0.0219, -0.1877, -0.1741, -0.1490],\n",
      "          [-0.0866,  0.0452, -0.1431,  0.0458, -0.1175],\n",
      "          [-0.1405,  0.1346,  0.1511, -0.1854, -0.1366],\n",
      "          [-0.1095, -0.1114, -0.1008,  0.1839, -0.0591]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1176, -0.1028,  0.1988, -0.0817, -0.0150],\n",
      "          [ 0.1538, -0.1620,  0.0233,  0.0813, -0.1602],\n",
      "          [ 0.1265, -0.0829,  0.1144,  0.1668, -0.0539],\n",
      "          [-0.1210,  0.1312, -0.0959, -0.0105,  0.1205],\n",
      "          [-0.0160,  0.1774, -0.1242, -0.1329, -0.1535]]],\n",
      "\n",
      "\n",
      "        [[[-0.1265,  0.1533, -0.0417, -0.1237,  0.0965],\n",
      "          [ 0.0884, -0.0186,  0.0079, -0.1489, -0.1696],\n",
      "          [ 0.0891, -0.0880, -0.0832,  0.1880,  0.1759],\n",
      "          [ 0.1442,  0.0126, -0.1202, -0.0170,  0.0432],\n",
      "          [-0.1886, -0.0143, -0.1024,  0.1467,  0.1232]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0314,  0.0240,  0.1634, -0.0672, -0.0532],\n",
      "          [-0.1676,  0.0303, -0.0094, -0.1490,  0.1117],\n",
      "          [ 0.0315,  0.0096, -0.1286, -0.0136, -0.0815],\n",
      "          [-0.1374, -0.1207,  0.0913,  0.0391, -0.0015],\n",
      "          [-0.0103, -0.1370,  0.0183,  0.0392,  0.0975]]],\n",
      "\n",
      "\n",
      "        [[[-0.0483,  0.0519, -0.0058,  0.0189, -0.1174],\n",
      "          [ 0.0462, -0.0780, -0.0599,  0.0112,  0.1347],\n",
      "          [ 0.0984,  0.1140, -0.0906, -0.1727,  0.0907],\n",
      "          [ 0.1641,  0.1185,  0.1957, -0.0676,  0.1687],\n",
      "          [-0.0162, -0.0926, -0.0085, -0.1415, -0.0388]]],\n",
      "\n",
      "\n",
      "        [[[-0.1036, -0.0583,  0.0769, -0.0652,  0.1137],\n",
      "          [-0.0463,  0.1175,  0.0850,  0.0853,  0.0254],\n",
      "          [-0.0856,  0.1141, -0.1028, -0.0783, -0.0793],\n",
      "          [-0.0620, -0.1171,  0.1849, -0.1871, -0.1474],\n",
      "          [ 0.1293,  0.0534,  0.0419,  0.1507,  0.1913]]]], requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "# 再次打印module中的属性张量named_buffers\n",
    "print(list(module.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('weight_mask', tensor([[[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]]]))]\n"
     ]
    }
   ],
   "source": [
    "# 再次打印module中的属性张量named_buffers\n",
    "print(list(module.named_buffers()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结论: 经过剪枝操作后, 原始的权重矩阵weight变成了weight_orig. 并且剪枝前打印为空列表的module.named_buffers(), 现在多了weight_mask参数."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.1686, -0.0279,  0.0093,  0.0203,  0.1026],\n",
      "          [-0.0616, -0.0219, -0.1877, -0.1741, -0.1490],\n",
      "          [-0.0866,  0.0452, -0.1431,  0.0458, -0.1175],\n",
      "          [-0.1405,  0.1346,  0.1511, -0.1854, -0.1366],\n",
      "          [-0.1095, -0.1114, -0.1008,  0.1839, -0.0591]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000, -0.0000,  0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000, -0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.1265,  0.1533, -0.0417, -0.1237,  0.0965],\n",
      "          [ 0.0884, -0.0186,  0.0079, -0.1489, -0.1696],\n",
      "          [ 0.0891, -0.0880, -0.0832,  0.1880,  0.1759],\n",
      "          [ 0.1442,  0.0126, -0.1202, -0.0170,  0.0432],\n",
      "          [-0.1886, -0.0143, -0.1024,  0.1467,  0.1232]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0314,  0.0240,  0.1634, -0.0672, -0.0532],\n",
      "          [-0.1676,  0.0303, -0.0094, -0.1490,  0.1117],\n",
      "          [ 0.0315,  0.0096, -0.1286, -0.0136, -0.0815],\n",
      "          [-0.1374, -0.1207,  0.0913,  0.0391, -0.0015],\n",
      "          [-0.0103, -0.1370,  0.0183,  0.0392,  0.0975]]],\n",
      "\n",
      "\n",
      "        [[[-0.0483,  0.0519, -0.0058,  0.0189, -0.1174],\n",
      "          [ 0.0462, -0.0780, -0.0599,  0.0112,  0.1347],\n",
      "          [ 0.0984,  0.1140, -0.0906, -0.1727,  0.0907],\n",
      "          [ 0.1641,  0.1185,  0.1957, -0.0676,  0.1687],\n",
      "          [-0.0162, -0.0926, -0.0085, -0.1415, -0.0388]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.0000,  0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 打印module.weight, 看看发现了什么？\n",
    "print(module.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结论: 经过剪枝操作后， 原始的weight变成了weight_orig，并存放在named_parameters中, 对应的剪枝矩阵存放在weight_mask中, 将weight_mask视作掩码张量, 再和weight_orig相乘的结果就存放在了weight中."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意:** 剪枝操作后的weight已经不再是module的参数(parameter), 而只是module的一个属性(attribute)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于每一次剪枝操作, 模型都会对应一个具体的_forward_pre_hooks函数用于剪枝，该函数存放执行过的剪枝操作."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([(213, <torch.nn.utils.prune.RandomStructured object at 0x00000235DA49B4F0>)])\n"
     ]
    }
   ],
   "source": [
    "# 打印_forward_pre_hooks\n",
    "print(module._forward_pre_hooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 范数结构化剪枝（ln_structured）\n",
    "一个模型的参数可以执行多次剪枝操作，这种操作被称为迭代剪枝（Iterative Pruning）。上述步骤已经对conv1进行了随机结构化剪枝，接下来对其再进行范数结构化剪枝，看看会发生什么？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " model state_dict keys:\n",
      "odict_keys(['conv1.bias', 'conv1.weight_orig', 'conv1.weight_mask', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])\n",
      "**************************************************\n",
      " module named_parameters:\n",
      "[('bias', Parameter containing:\n",
      "tensor([ 0.0585, -0.0013, -0.1102,  0.1541,  0.1304, -0.1803],\n",
      "       requires_grad=True)), ('weight_orig', Parameter containing:\n",
      "tensor([[[[ 0.1686, -0.0279,  0.0093,  0.0203,  0.1026],\n",
      "          [-0.0616, -0.0219, -0.1877, -0.1741, -0.1490],\n",
      "          [-0.0866,  0.0452, -0.1431,  0.0458, -0.1175],\n",
      "          [-0.1405,  0.1346,  0.1511, -0.1854, -0.1366],\n",
      "          [-0.1095, -0.1114, -0.1008,  0.1839, -0.0591]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1176, -0.1028,  0.1988, -0.0817, -0.0150],\n",
      "          [ 0.1538, -0.1620,  0.0233,  0.0813, -0.1602],\n",
      "          [ 0.1265, -0.0829,  0.1144,  0.1668, -0.0539],\n",
      "          [-0.1210,  0.1312, -0.0959, -0.0105,  0.1205],\n",
      "          [-0.0160,  0.1774, -0.1242, -0.1329, -0.1535]]],\n",
      "\n",
      "\n",
      "        [[[-0.1265,  0.1533, -0.0417, -0.1237,  0.0965],\n",
      "          [ 0.0884, -0.0186,  0.0079, -0.1489, -0.1696],\n",
      "          [ 0.0891, -0.0880, -0.0832,  0.1880,  0.1759],\n",
      "          [ 0.1442,  0.0126, -0.1202, -0.0170,  0.0432],\n",
      "          [-0.1886, -0.0143, -0.1024,  0.1467,  0.1232]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0314,  0.0240,  0.1634, -0.0672, -0.0532],\n",
      "          [-0.1676,  0.0303, -0.0094, -0.1490,  0.1117],\n",
      "          [ 0.0315,  0.0096, -0.1286, -0.0136, -0.0815],\n",
      "          [-0.1374, -0.1207,  0.0913,  0.0391, -0.0015],\n",
      "          [-0.0103, -0.1370,  0.0183,  0.0392,  0.0975]]],\n",
      "\n",
      "\n",
      "        [[[-0.0483,  0.0519, -0.0058,  0.0189, -0.1174],\n",
      "          [ 0.0462, -0.0780, -0.0599,  0.0112,  0.1347],\n",
      "          [ 0.0984,  0.1140, -0.0906, -0.1727,  0.0907],\n",
      "          [ 0.1641,  0.1185,  0.1957, -0.0676,  0.1687],\n",
      "          [-0.0162, -0.0926, -0.0085, -0.1415, -0.0388]]],\n",
      "\n",
      "\n",
      "        [[[-0.1036, -0.0583,  0.0769, -0.0652,  0.1137],\n",
      "          [-0.0463,  0.1175,  0.0850,  0.0853,  0.0254],\n",
      "          [-0.0856,  0.1141, -0.1028, -0.0783, -0.0793],\n",
      "          [-0.0620, -0.1171,  0.1849, -0.1871, -0.1474],\n",
      "          [ 0.1293,  0.0534,  0.0419,  0.1507,  0.1913]]]], requires_grad=True))]\n",
      "**************************************************\n",
      " module named_buffers:\n",
      "[('weight_mask', tensor([[[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]]]))]\n",
      "**************************************************\n",
      " module weight:\n",
      "tensor([[[[ 0.1686, -0.0279,  0.0093,  0.0203,  0.1026],\n",
      "          [-0.0616, -0.0219, -0.1877, -0.1741, -0.1490],\n",
      "          [-0.0866,  0.0452, -0.1431,  0.0458, -0.1175],\n",
      "          [-0.1405,  0.1346,  0.1511, -0.1854, -0.1366],\n",
      "          [-0.1095, -0.1114, -0.1008,  0.1839, -0.0591]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000, -0.0000,  0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000, -0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.1265,  0.1533, -0.0417, -0.1237,  0.0965],\n",
      "          [ 0.0884, -0.0186,  0.0079, -0.1489, -0.1696],\n",
      "          [ 0.0891, -0.0880, -0.0832,  0.1880,  0.1759],\n",
      "          [ 0.1442,  0.0126, -0.1202, -0.0170,  0.0432],\n",
      "          [-0.1886, -0.0143, -0.1024,  0.1467,  0.1232]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.0000, -0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000, -0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000, -0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.0000,  0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "**************************************************\n",
      " module _forward_pre_hooks:\n",
      "OrderedDict([(214, <torch.nn.utils.prune.PruningContainer object at 0x00000235DA49BD60>)])\n"
     ]
    }
   ],
   "source": [
    "# 第一个参数: module, 代表要进行剪枝的特定模块, 这里指的是module=model.conv1,\n",
    "#             说明这里要对第一个卷积层执行剪枝.\n",
    "# 第二个参数: name, 代表要对选中的模块中的哪些参数执行剪枝.\n",
    "#             这里设定为name=\"weight\", 说明是对网络中的weight剪枝, 而不对bias剪枝.\n",
    "# 第三个参数: amount, 代表要对模型中特定比例或绝对数量的参数执行剪枝.\n",
    "#             amount是一个介于0.0-1.0的float数值,代表比例, 或者一个正整数，代表指定剪裁掉多少个参数.\n",
    "# 第四个参数: n, 代表范数类型，这里n=2代表是L2范数.\n",
    "# 第五个参数: dim, 代表要进行剪枝通道(channel)的维度索引.\n",
    "\n",
    "prune.ln_structured(module, name=\"weight\", amount=0.5, n=2, dim=0)\n",
    "\n",
    "# 再次打印模型参数\n",
    "print(\" model state_dict keys:\")\n",
    "print(model.state_dict().keys())\n",
    "print('*'*50)\n",
    "\n",
    "print(\" module named_parameters:\")\n",
    "print(list(module.named_parameters()))\n",
    "print('*'*50)\n",
    "\n",
    "print(\" module named_buffers:\")\n",
    "print(list(module.named_buffers()))\n",
    "print('*'*50)\n",
    "\n",
    "print(\" module weight:\")\n",
    "print(module.weight)\n",
    "print('*'*50)\n",
    "\n",
    "print(\" module _forward_pre_hooks:\")\n",
    "print(module._forward_pre_hooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结论：迭代剪枝相当于把多个剪枝核序列化成一个剪枝核, 新的 mask 矩阵与旧的 mask 矩阵的结合使用 PruningContainer 中的 compute_mask 方法，最后只有一个weight_orig和weight_mask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3 随机非结构化剪枝（random_unstructured）\n",
    "可以对模型的任意子结构进行剪枝操作, 除了在weight上面剪枝, 还可以对bias进行剪枝."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " model state_dict keys:\n",
      "odict_keys(['conv1.weight_orig', 'conv1.bias_orig', 'conv1.weight_mask', 'conv1.bias_mask', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])\n",
      "**************************************************\n",
      " module named_parameters:\n",
      "[('weight_orig', Parameter containing:\n",
      "tensor([[[[ 0.1686, -0.0279,  0.0093,  0.0203,  0.1026],\n",
      "          [-0.0616, -0.0219, -0.1877, -0.1741, -0.1490],\n",
      "          [-0.0866,  0.0452, -0.1431,  0.0458, -0.1175],\n",
      "          [-0.1405,  0.1346,  0.1511, -0.1854, -0.1366],\n",
      "          [-0.1095, -0.1114, -0.1008,  0.1839, -0.0591]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1176, -0.1028,  0.1988, -0.0817, -0.0150],\n",
      "          [ 0.1538, -0.1620,  0.0233,  0.0813, -0.1602],\n",
      "          [ 0.1265, -0.0829,  0.1144,  0.1668, -0.0539],\n",
      "          [-0.1210,  0.1312, -0.0959, -0.0105,  0.1205],\n",
      "          [-0.0160,  0.1774, -0.1242, -0.1329, -0.1535]]],\n",
      "\n",
      "\n",
      "        [[[-0.1265,  0.1533, -0.0417, -0.1237,  0.0965],\n",
      "          [ 0.0884, -0.0186,  0.0079, -0.1489, -0.1696],\n",
      "          [ 0.0891, -0.0880, -0.0832,  0.1880,  0.1759],\n",
      "          [ 0.1442,  0.0126, -0.1202, -0.0170,  0.0432],\n",
      "          [-0.1886, -0.0143, -0.1024,  0.1467,  0.1232]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0314,  0.0240,  0.1634, -0.0672, -0.0532],\n",
      "          [-0.1676,  0.0303, -0.0094, -0.1490,  0.1117],\n",
      "          [ 0.0315,  0.0096, -0.1286, -0.0136, -0.0815],\n",
      "          [-0.1374, -0.1207,  0.0913,  0.0391, -0.0015],\n",
      "          [-0.0103, -0.1370,  0.0183,  0.0392,  0.0975]]],\n",
      "\n",
      "\n",
      "        [[[-0.0483,  0.0519, -0.0058,  0.0189, -0.1174],\n",
      "          [ 0.0462, -0.0780, -0.0599,  0.0112,  0.1347],\n",
      "          [ 0.0984,  0.1140, -0.0906, -0.1727,  0.0907],\n",
      "          [ 0.1641,  0.1185,  0.1957, -0.0676,  0.1687],\n",
      "          [-0.0162, -0.0926, -0.0085, -0.1415, -0.0388]]],\n",
      "\n",
      "\n",
      "        [[[-0.1036, -0.0583,  0.0769, -0.0652,  0.1137],\n",
      "          [-0.0463,  0.1175,  0.0850,  0.0853,  0.0254],\n",
      "          [-0.0856,  0.1141, -0.1028, -0.0783, -0.0793],\n",
      "          [-0.0620, -0.1171,  0.1849, -0.1871, -0.1474],\n",
      "          [ 0.1293,  0.0534,  0.0419,  0.1507,  0.1913]]]], requires_grad=True)), ('bias_orig', Parameter containing:\n",
      "tensor([ 0.0585, -0.0013, -0.1102,  0.1541,  0.1304, -0.1803],\n",
      "       requires_grad=True))]\n",
      "**************************************************\n",
      " module named_buffers:\n",
      "[('weight_mask', tensor([[[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]]])), ('bias_mask', tensor([1., 0., 1., 1., 1., 1.]))]\n",
      "**************************************************\n",
      " module bias:\n",
      "tensor([ 0.0585, -0.0000, -0.1102,  0.1541,  0.1304, -0.1803],\n",
      "       grad_fn=<MulBackward0>)\n",
      "**************************************************\n",
      " module _forward_pre_hooks:\n",
      "OrderedDict([(214, <torch.nn.utils.prune.PruningContainer object at 0x00000235DA49BD60>), (215, <torch.nn.utils.prune.RandomUnstructured object at 0x00000235D9324790>)])\n"
     ]
    }
   ],
   "source": [
    "# 第一个参数: module, 代表要进行剪枝的特定模块, 这里指的是module=model.conv1,\n",
    "#             说明这里要对第一个卷积层执行剪枝.\n",
    "# 第二个参数: name, 代表要对选中的模块中的哪些参数执行剪枝.\n",
    "#             这里设定为name=\"weight\", 说明是对网络中的weight剪枝, 而不对bias剪枝.\n",
    "# 第三个参数: amount, 代表要对模型中特定比例或绝对数量的参数执行剪枝.\n",
    "#             amount是一个介于0.0-1.0的float数值,代表比例, 或者一个正整数，代表指定剪裁掉多少个参数.\n",
    "\n",
    "prune.random_unstructured(module, name=\"bias\", amount=1)\n",
    "\n",
    "# 再次打印模型参数\n",
    "print(\" model state_dict keys:\")\n",
    "print(model.state_dict().keys())\n",
    "print('*'*50)\n",
    "\n",
    "print(\" module named_parameters:\")\n",
    "print(list(module.named_parameters()))\n",
    "print('*'*50)\n",
    "\n",
    "print(\" module named_buffers:\")\n",
    "print(list(module.named_buffers()))\n",
    "print('*'*50)\n",
    "\n",
    "print(\" module bias:\")\n",
    "print(module.bias)\n",
    "print('*'*50)\n",
    "\n",
    "print(\" module _forward_pre_hooks:\")\n",
    "print(module._forward_pre_hooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结论: 在module的不同参数集合上应用不同的剪枝策略, 可以发现在模型参数state_dict和named_parameters中不仅仅有了weight_orig, 也有了bias_orig. 在参数named_buffers中, 也同时出现了weight_mask和bias_mask. \n",
    "最后, 因为我们在两类参数上应用了两种不同的剪枝函数, 因此_forward_pre_hooks中也打印出了2个不同的函数结果."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.4 范数非结构化剪枝（l1_unstructured）\n",
    "前面介绍了对指定的conv1层的weight和bias进行了不同方法的剪枝，那么能不能支持同时对多层网络的特定参数进行剪枝呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "random_structured() missing 1 required positional argument: 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[227], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# 对模型中所有全连接层执行ln_structured剪枝操作, 选取40%的参数剪枝\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear):\n\u001b[1;32m----> 8\u001b[0m         \u001b[43mprune\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_structured\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mamount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# 再次打印模型参数\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m model state_dict keys:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: random_structured() missing 1 required positional argument: 'dim'"
     ]
    }
   ],
   "source": [
    "# 对于模型进行分模块参数的剪枝\n",
    "for name, module in model.named_modules():\n",
    "    # 对模型中所有的卷积层执行l1_unstructured剪枝操作, 选取20%的参数剪枝\n",
    "    if isinstance(module, torch.nn.Conv2d):\n",
    "        prune.l1_unstructured(module, name=\"bias\", amount=0.2)\n",
    "    # 对模型中所有全连接层执行ln_structured剪枝操作, 选取40%的参数剪枝\n",
    "    elif isinstance(module, torch.nn.Linear):\n",
    "        prune.random_structured(module, name=\"weight\", amount=0.4,dim=0)\n",
    "\n",
    "# 再次打印模型参数\n",
    "print(\" model state_dict keys:\")\n",
    "print(model.state_dict().keys())\n",
    "print('*'*50)\n",
    "\n",
    "print(\" module named_parameters:\")\n",
    "print(list(module.named_parameters()))\n",
    "print('*'*50)\n",
    "\n",
    "print(\" module named_buffers:\")\n",
    "print(list(module.named_buffers()))\n",
    "print('*'*50)\n",
    "\n",
    "print(\" module weight:\")\n",
    "print(module.weight)\n",
    "print('*'*50)\n",
    "\n",
    "print(\" module bias:\")\n",
    "print(module.bias)\n",
    "print('*'*50)\n",
    "\n",
    "print(\" module _forward_pre_hooks:\")\n",
    "print(module._forward_pre_hooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "怎么能看到所有的剪枝历史呢？ module._forward_pre_hooks是一个用于在模型的前向传播之前执行自定义操作的机制，这里记录了执行过的剪枝方法."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'LnStructured' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[212], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hook\u001b[38;5;241m.\u001b[39m_tensor_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhook\u001b[49m\u001b[43m)\u001b[49m)  \n",
      "\u001b[1;31mTypeError\u001b[0m: 'LnStructured' object is not iterable"
     ]
    }
   ],
   "source": [
    "# 打印剪枝历史\n",
    "for hook in module._forward_pre_hooks.values():\n",
    "    if hook._tensor_name == \"weight\":  \n",
    "        break\n",
    "\n",
    "print(list(hook))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来对模型进行剪枝永久化操作remove，经过前面的剪枝步骤， 原来的weight 已经变成'weight_orig', 而 weight 是'weight_orig' 与 mask 矩阵相乘后的结果，变成一个属性,请观察在remove之后发生了哪些变化？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      " model state_dict keys:\n",
      "odict_keys(['conv1.bias_orig', 'conv1.weight', 'conv1.bias_mask', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])\n",
      "**************************************************\n",
      " model named_parameters:\n",
      "[('bias_orig', Parameter containing:\n",
      "tensor([-0.0873, -0.0570, -0.0998, -0.1480,  0.0678, -0.0647],\n",
      "       requires_grad=True)), ('weight', Parameter containing:\n",
      "tensor([[[[ 0.0000,  0.0000,  0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000, -0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.0000, -0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000, -0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.0000,  0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000, -0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0340, -0.1810,  0.1120,  0.1799,  0.1920],\n",
      "          [-0.1283, -0.0378,  0.1094,  0.1955,  0.1841],\n",
      "          [-0.1118,  0.1328, -0.1682,  0.0657, -0.1589],\n",
      "          [-0.1726,  0.1043,  0.1747,  0.0133,  0.0782],\n",
      "          [ 0.0586,  0.0529,  0.1075, -0.1586,  0.0440]]],\n",
      "\n",
      "\n",
      "        [[[-0.0737,  0.1753,  0.1738, -0.1272, -0.1745],\n",
      "          [-0.1371,  0.1608,  0.1895, -0.1511,  0.0777],\n",
      "          [-0.1236,  0.1698, -0.1161,  0.1586,  0.1400],\n",
      "          [-0.0262,  0.0552, -0.0688, -0.0593, -0.0427],\n",
      "          [ 0.1855, -0.1794,  0.1887, -0.1575,  0.1460]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000, -0.0000, -0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000, -0.0000, -0.0000, -0.0000]]]], requires_grad=True))]\n",
      "**************************************************\n",
      " model named_buffers:\n",
      "[('bias_mask', tensor([1., 1., 1., 0., 1., 1.]))]\n",
      "**************************************************\n",
      " model forward_pre_hooks:\n",
      "OrderedDict([(173, <torch.nn.utils.prune.RandomUnstructured object at 0x00000235DA3C04C0>)])\n"
     ]
    }
   ],
   "source": [
    "# 对module执行剪枝永久化操作remove\n",
    "prune.remove(module, 'weight')\n",
    "print('*'*50)\n",
    "\n",
    "# 将剪枝后的模型的状态字典打印出来\n",
    "print(\" model state_dict keys:\")\n",
    "print(model.state_dict().keys())\n",
    "print('*'*50)\n",
    "\n",
    "# 再次打印模型参数\n",
    "print(\" model named_parameters:\")\n",
    "print(list(module.named_parameters()))\n",
    "print('*'*50)\n",
    "\n",
    "# 再次打印模型mask buffers参数\n",
    "print(\" model named_buffers:\")\n",
    "print(list(module.named_buffers()))\n",
    "print('*'*50)\n",
    "\n",
    "# 再次打印模型的_forward_pre_hooks\n",
    "print(\" model forward_pre_hooks:\")\n",
    "print(module._forward_pre_hooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结论: 对模型的weight执行remove操作后, 模型参数集合中weight_orig消失, 变成了weight, 说明针对weight的剪枝已经永久化生效. 对于named_buffers张量打印可以看出, 只剩下[], 因为针对weight做掩码的weight_mask已经生效完毕, 不再需要保留了. \n",
    "同理, 在_forward_pre_hooks中也只剩下空字典了.weight 又变成了 parameters, 剪枝变成永久化."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.多参数模块的剪枝(Pruning multiple parameters).¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])\n",
      "**************************************************\n",
      "dict_keys([])\n",
      "**************************************************\n",
      "dict_keys(['conv1.weight_mask', 'conv2.weight_mask', 'fc1.weight_mask', 'fc2.weight_mask', 'fc3.weight_mask'])\n",
      "**************************************************\n",
      "odict_keys(['conv1.bias', 'conv1.weight_orig', 'conv1.weight_mask', 'conv2.bias', 'conv2.weight_orig', 'conv2.weight_mask', 'fc1.bias', 'fc1.weight_orig', 'fc1.weight_mask', 'fc2.bias', 'fc2.weight_orig', 'fc2.weight_mask', 'fc3.bias', 'fc3.weight_orig', 'fc3.weight_mask'])\n"
     ]
    }
   ],
   "source": [
    "model = LeNet().to(device=device)\n",
    "\n",
    "# 打印初始模型的所有状态字典\n",
    "print(model.state_dict().keys())\n",
    "print('*'*50)\n",
    "\n",
    "# 打印初始模型的mask buffers张量字典名称\n",
    "print(dict(model.named_buffers()).keys())\n",
    "print('*'*50)\n",
    "\n",
    "# 对于模型进行分模块参数的剪枝\n",
    "for name, module in model.named_modules():\n",
    "    # 对模型中所有的卷积层执行l1_unstructured剪枝操作, 选取20%的参数剪枝\n",
    "    if isinstance(module, torch.nn.Conv2d):\n",
    "        prune.l1_unstructured(module, name=\"weight\", amount=0.2)\n",
    "    # 对模型中所有全连接层执行ln_structured剪枝操作, 选取40%的参数剪枝\n",
    "    elif isinstance(module, torch.nn.Linear):\n",
    "        prune.ln_structured(module, name=\"weight\", amount=0.4, n=2, dim=0)\n",
    "\n",
    "# 打印多参数模块剪枝后的mask buffers张量字典名称\n",
    "print(dict(model.named_buffers()).keys())\n",
    "print('*'*50)\n",
    "\n",
    "# 打印多参数模块剪枝后模型的所有状态字典名称\n",
    "print(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.全局剪枝(GLobal pruning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更普遍也更通用的剪枝策略是采用全局剪枝(global pruning), 比如在整体网络的视角下剪枝掉20%的权重参数, 而不是在每一层上都剪枝掉20%的权重参数. 采用全局剪枝后, 不同的层被剪掉的百分比不同."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])\n",
      "**************************************************\n",
      "odict_keys(['conv1.bias', 'conv1.weight_orig', 'conv1.weight_mask', 'conv2.bias', 'conv2.weight_orig', 'conv2.weight_mask', 'fc1.bias', 'fc1.weight_orig', 'fc1.weight_mask', 'fc2.bias', 'fc2.weight_orig', 'fc2.weight_mask', 'fc3.weight', 'fc3.bias'])\n"
     ]
    }
   ],
   "source": [
    "model = LeNet().to(device=device)\n",
    "\n",
    "# 首先打印初始化模型的状态字典\n",
    "print(model.state_dict().keys())\n",
    "print('*'*50)\n",
    "\n",
    "# 构建参数集合, 决定哪些层, 哪些参数集合参与剪枝\n",
    "parameters_to_prune = (\n",
    "            (model.conv1, 'weight'),\n",
    "            (model.conv2, 'weight'),\n",
    "            (model.fc1, 'weight'),\n",
    "            (model.fc2, 'weight'))\n",
    "\n",
    "# 调用prune中的全局剪枝函数global_unstructured执行剪枝操作, 此处针对整体模型中的20%参数量进行剪枝\n",
    "prune.global_unstructured(parameters_to_prune, pruning_method=prune.L1Unstructured, amount=0.2)\n",
    "\n",
    "# 最后打印剪枝后的模型的状态字典\n",
    "print(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "针对模型剪枝后, 不同的层会有不同比例的权重参数被剪掉, 利用代码打印出来看看:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity in conv1.weight: 7.33%\n",
      "Sparsity in conv2.weight: 16.83%\n",
      "Sparsity in fc1.weight: 21.90%\n",
      "Sparsity in fc2.weight: 15.14%\n",
      "Global sparsity: 20.00%\n"
     ]
    }
   ],
   "source": [
    "model = LeNet().to(device=device)\n",
    "\n",
    "parameters_to_prune = (\n",
    "            (model.conv1, 'weight'),\n",
    "            (model.conv2, 'weight'),\n",
    "            (model.fc1, 'weight'),\n",
    "            (model.fc2, 'weight'))\n",
    "\n",
    "prune.global_unstructured(parameters_to_prune, pruning_method=prune.L1Unstructured, amount=0.2)\n",
    "\n",
    "print(\n",
    "    \"Sparsity in conv1.weight: {:.2f}%\".format(\n",
    "    100. * float(torch.sum(model.conv1.weight == 0))\n",
    "    / float(model.conv1.weight.nelement())\n",
    "    ))\n",
    "\n",
    "print(\n",
    "    \"Sparsity in conv2.weight: {:.2f}%\".format(\n",
    "    100. * float(torch.sum(model.conv2.weight == 0))\n",
    "    / float(model.conv2.weight.nelement())\n",
    "    ))\n",
    "\n",
    "print(\n",
    "    \"Sparsity in fc1.weight: {:.2f}%\".format(\n",
    "    100. * float(torch.sum(model.fc1.weight == 0))\n",
    "    / float(model.fc1.weight.nelement())\n",
    "    ))\n",
    "\n",
    "print(\n",
    "    \"Sparsity in fc2.weight: {:.2f}%\".format(\n",
    "    100. * float(torch.sum(model.fc2.weight == 0))\n",
    "    / float(model.fc2.weight.nelement())\n",
    "    ))\n",
    "\n",
    "\n",
    "print(\n",
    "    \"Global sparsity: {:.2f}%\".format(\n",
    "    100. * float(torch.sum(model.conv1.weight == 0)\n",
    "               + torch.sum(model.conv2.weight == 0)\n",
    "               + torch.sum(model.fc1.weight == 0)\n",
    "               + torch.sum(model.fc2.weight == 0))\n",
    "         / float(model.conv1.weight.nelement()\n",
    "               + model.conv2.weight.nelement()\n",
    "               + model.fc1.weight.nelement()\n",
    "               + model.fc2.weight.nelement())\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结论: 当采用全局剪枝策略的时候(假定20%比例参数参与剪枝), 仅保证模型总体参数量的20%被剪枝掉, 具体到每一层的情况则由模型的具体参数分布情况来定.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.用户自定义剪枝(Custom pruning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "剪枝模型通过继承class BasePruningMethod()来执行剪枝, 内部有若干方法: call, apply_mask, apply, prune, remove等等. 一般来说, 用户只需要实现__init__, 和compute_mask两个函数即可完成自定义的剪枝规则设定."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义剪枝方法的类, 一定要继承prune.BasePruningMethod\n",
    "class custom_prune(prune.BasePruningMethod):\n",
    "    PRUNING_TYPE = \"unstructured\"\n",
    "\n",
    "    # 内部实现compute_mask函数, 完成程序员自己定义的剪枝规则, 本质上就是如何去mask掉权重参数\n",
    "    def compute_mask(self, t, default_mask):\n",
    "        mask = default_mask.clone()\n",
    "        # 此处定义的规则是每隔一个参数就遮掩掉一个, 最终参与剪枝的参数量的50%被mask掉\n",
    "        mask.view(-1)[::2] = 0\n",
    "        return mask\n",
    "\n",
    "# 自定义剪枝方法的函数, 内部直接调用剪枝类的方法apply\n",
    "def custome_unstructured_pruning(module, name):\n",
    "    custom_prune.apply(module, name)\n",
    "    return module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.])\n",
      "3.9970874786376953 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# 实例化模型类\n",
    "model = LeNet().to(device=device)\n",
    "\n",
    "start = time.time()\n",
    "# 调用自定义剪枝方法的函数, 对model中的第2个全连接层fc2中的偏置bias执行自定义剪枝\n",
    "custome_unstructured_pruning(model.fc2, name=\"bias\")\n",
    "\n",
    "# 剪枝成功的最大标志, 就是拥有了bias_mask参数\n",
    "print(model.fc2.bias_mask)\n",
    "\n",
    "# 打印一下自定义剪枝的耗时\n",
    "duration = time.time() - start\n",
    "print(duration * 1000, 'ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结论: 打印出来的bias_mask张量, 完全是按照预定义的方式每隔一位遮掩掉一位, 0和1交替出现, 后续执行remove操作的时候, 原始的bias_orig中的权重就会同样的被每隔一位剪枝掉一位. 在GPU机器上执行自定义剪枝速度特别快, 仅需1.7ms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
