{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建一个经典的LeNet网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 1: 图像的输入通道(1是黑白图像), 6: 输出通道, 3x3: 卷积核的尺寸\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5x5 是经历卷积操作后的图片尺寸\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, int(x.nelement() / x.shape[0]))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = LeNet().to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('weight', Parameter containing:\n",
      "tensor([[[[-0.3249,  0.0024,  0.1380],\n",
      "          [ 0.3285,  0.0024,  0.2320],\n",
      "          [ 0.3222,  0.0152, -0.0489]]],\n",
      "\n",
      "\n",
      "        [[[-0.0894,  0.1978, -0.1947],\n",
      "          [-0.0299, -0.0440,  0.2470],\n",
      "          [-0.0769, -0.1188, -0.2219]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2724, -0.1438, -0.2627],\n",
      "          [ 0.2301, -0.0314, -0.0930],\n",
      "          [-0.0545,  0.1504, -0.0504]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0624, -0.0672, -0.1851],\n",
      "          [ 0.2783,  0.2560,  0.2220],\n",
      "          [-0.2856, -0.0847, -0.0672]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1904, -0.2472,  0.1982],\n",
      "          [-0.2487, -0.0719,  0.0186],\n",
      "          [ 0.1364,  0.0674,  0.2810]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0624,  0.1727,  0.3053],\n",
      "          [-0.1114,  0.0935, -0.3089],\n",
      "          [-0.3035, -0.1825,  0.1328]]]], requires_grad=True)), ('bias', Parameter containing:\n",
      "tensor([-0.0134,  0.1742,  0.0337,  0.2687,  0.2862,  0.1170],\n",
      "       requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "module = model.conv1\n",
    "print(list(module.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(list(module.named_buffers()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第一个参数: module, 代表要进行剪枝的特定模块, 之前我们已经制定了module=model.conv1,\n",
    "#             说明这里要对第一个卷积层执行剪枝.\n",
    "# 第二个参数: name, 指定要对选中的模块中的哪些参数执行剪枝.\n",
    "#             这里设定为name=\"weight\", 意味着对连接网络中的weight剪枝, 而不对bias剪枝.\n",
    "# 第三个参数: amount, 指定要对模型中多大比例的参数执行剪枝.\n",
    "#             amount是一个介于0.0-1.0的float数值, 或者一个正整数指定剪裁掉多少条连接边.\n",
    "\n",
    "prune.random_unstructured(module, name=\"weight\", amount=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bias', Parameter containing:\n",
      "tensor([-0.0134,  0.1742,  0.0337,  0.2687,  0.2862,  0.1170],\n",
      "       requires_grad=True)), ('weight_orig', Parameter containing:\n",
      "tensor([[[[-0.3249,  0.0024,  0.1380],\n",
      "          [ 0.3285,  0.0024,  0.2320],\n",
      "          [ 0.3222,  0.0152, -0.0489]]],\n",
      "\n",
      "\n",
      "        [[[-0.0894,  0.1978, -0.1947],\n",
      "          [-0.0299, -0.0440,  0.2470],\n",
      "          [-0.0769, -0.1188, -0.2219]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2724, -0.1438, -0.2627],\n",
      "          [ 0.2301, -0.0314, -0.0930],\n",
      "          [-0.0545,  0.1504, -0.0504]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0624, -0.0672, -0.1851],\n",
      "          [ 0.2783,  0.2560,  0.2220],\n",
      "          [-0.2856, -0.0847, -0.0672]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1904, -0.2472,  0.1982],\n",
      "          [-0.2487, -0.0719,  0.0186],\n",
      "          [ 0.1364,  0.0674,  0.2810]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0624,  0.1727,  0.3053],\n",
      "          [-0.1114,  0.0935, -0.3089],\n",
      "          [-0.3035, -0.1825,  0.1328]]]], requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "print(list(module.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('weight_mask', tensor([[[[0., 1., 1.],\n",
      "          [1., 1., 0.],\n",
      "          [0., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 0., 1.],\n",
      "          [0., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.],\n",
      "          [0., 1., 0.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 1.],\n",
      "          [1., 0., 1.],\n",
      "          [1., 1., 1.]]]]))]\n"
     ]
    }
   ],
   "source": [
    "print(list(module.named_buffers()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结论: 模型经历剪枝操作后, 原始的权重矩阵weight参数不见了, 变成了weight_orig. 并且刚刚打印为空列表的module.named_buffers(), 此时拥有了一个weight_mask参数."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这时打印module.weight属性值, 看看有什么启发?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.0000,  0.0024,  0.1380],\n",
      "          [ 0.3285,  0.0024,  0.0000],\n",
      "          [ 0.0000,  0.0152, -0.0489]]],\n",
      "\n",
      "\n",
      "        [[[-0.0894,  0.1978, -0.1947],\n",
      "          [-0.0299, -0.0000,  0.2470],\n",
      "          [-0.0000, -0.1188, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2724, -0.0000, -0.0000],\n",
      "          [ 0.2301, -0.0000, -0.0000],\n",
      "          [-0.0545,  0.1504, -0.0504]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0624, -0.0672, -0.0000],\n",
      "          [ 0.0000,  0.2560,  0.0000],\n",
      "          [-0.2856, -0.0847, -0.0672]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1904, -0.0000,  0.1982],\n",
      "          [-0.2487, -0.0719,  0.0186],\n",
      "          [ 0.1364,  0.0674,  0.2810]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0624,  0.0000,  0.3053],\n",
      "          [-0.1114,  0.0000, -0.3089],\n",
      "          [-0.3035, -0.1825,  0.1328]]]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(module.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结论: 经过剪枝操作后的模型, 原始的参数存放在了weight_orig中, 对应的剪枝矩阵存放在weight_mask中, 而将weight_mask视作掩码张量, 再和weight_orig相乘的结果就存放在了weight中."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意: 剪枝操作后的weight已经不再是module的参数(parameter), 而只是module的一个属性(attribute)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于每一次剪枝操作, 模型都会对应一个具体的_forward_pre_hooks函数用于剪枝."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([(0, <torch.nn.utils.prune.RandomUnstructured object at 0x000001A1BFDC8FD0>)])\n"
     ]
    }
   ],
   "source": [
    "print(module._forward_pre_hooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('weight_orig', Parameter containing:\n",
      "tensor([[[[-0.3249,  0.0024,  0.1380],\n",
      "          [ 0.3285,  0.0024,  0.2320],\n",
      "          [ 0.3222,  0.0152, -0.0489]]],\n",
      "\n",
      "\n",
      "        [[[-0.0894,  0.1978, -0.1947],\n",
      "          [-0.0299, -0.0440,  0.2470],\n",
      "          [-0.0769, -0.1188, -0.2219]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2724, -0.1438, -0.2627],\n",
      "          [ 0.2301, -0.0314, -0.0930],\n",
      "          [-0.0545,  0.1504, -0.0504]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0624, -0.0672, -0.1851],\n",
      "          [ 0.2783,  0.2560,  0.2220],\n",
      "          [-0.2856, -0.0847, -0.0672]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1904, -0.2472,  0.1982],\n",
      "          [-0.2487, -0.0719,  0.0186],\n",
      "          [ 0.1364,  0.0674,  0.2810]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0624,  0.1727,  0.3053],\n",
      "          [-0.1114,  0.0935, -0.3089],\n",
      "          [-0.3035, -0.1825,  0.1328]]]], requires_grad=True)), ('bias_orig', Parameter containing:\n",
      "tensor([-0.0134,  0.1742,  0.0337,  0.2687,  0.2862,  0.1170],\n",
      "       requires_grad=True))]\n",
      "**************************************************\n",
      "[('weight_mask', tensor([[[[0., 1., 1.],\n",
      "          [1., 1., 0.],\n",
      "          [0., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 0., 1.],\n",
      "          [0., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.],\n",
      "          [0., 1., 0.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 1.],\n",
      "          [1., 0., 1.],\n",
      "          [1., 1., 1.]]]])), ('bias_mask', tensor([0., 1., 0., 1., 1., 0.]))]\n",
      "**************************************************\n",
      "tensor([-0.0000, 0.1742, 0.0000, 0.2687, 0.2862, 0.0000],\n",
      "       grad_fn=<MulBackward0>)\n",
      "**************************************************\n",
      "OrderedDict([(0, <torch.nn.utils.prune.RandomUnstructured object at 0x000001A1BFDC8FD0>), (1, <torch.nn.utils.prune.L1Unstructured object at 0x000001A1BFDDE400>)])\n"
     ]
    }
   ],
   "source": [
    "# 第一个参数: module, 代表剪枝的对象, 此处代表LeNet中的conv1\n",
    "# 第二个参数: name, 代表剪枝对象中的具体参数, 此处代表偏置量\n",
    "# 第三个参数: amount, 代表剪枝的数量, 可以设置为0.0-1.0之间表示比例, 也可以用正整数表示剪枝的参数绝对数量\n",
    "prune.l1_unstructured(module, name=\"bias\", amount=3)\n",
    "\n",
    "# 再次打印模型参数\n",
    "print(list(module.named_parameters()))\n",
    "print('*'*50)\n",
    "print(list(module.named_buffers()))\n",
    "print('*'*50)\n",
    "print(module.bias)\n",
    "print('*'*50)\n",
    "print(module._forward_pre_hooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结论: 在module的不同参数集合上应用不同的剪枝策略, 我们发现模型参数中不仅仅有了weight_orig, 也有了bias_orig. 在起到掩码张量作用的named_buffers中, 也同时出现了weight_mask和bias_mask. 最后, 因为我们在两类参数上应用了两种不同的剪枝函数, 因此_forward_pre_hooks中也打印出了2个不同的函数结果."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 序列化一个剪枝模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
