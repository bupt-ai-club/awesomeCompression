# 第3章 模型剪枝

## 介绍

剪枝是模型压缩中一种重要的技术，其基本思想是将模型中不重要的权重和分支裁剪掉，将网络结构稀疏化，进而得到参数量更小的模型，降低内存开销，使得推理速度更快。

## 为何剪枝（Why Pruning?）

彩票假设(lottery ticket hypothesis)：在随机初始化的密集神经网络中，存在一些子网络（称为“中奖票”），这些子网络在单独训练时可以达到与原始网络相似的测试精度，且所需的迭代次数相同。


## 何为剪枝（What is Pruning?）

按照剪枝粒度进行划分，剪枝可分为细粒度剪枝（Fine-grained Pruning）、基于模式的剪枝（Pattern-based Pruning）、向量级剪枝（Vector-level Pruning）、内核级剪枝（Kernel-level Pruning）与通道级剪枝（Channel-level Pruning）。

如下图所示，展示了从细粒度剪枝到通道级的剪枝，剪枝越来越规则和结构化。

![granularities](./images/granularities.png)


### 细粒度剪枝



### 基于模式的剪枝



### 向量级剪枝


### 内核级剪枝


### 通道级剪枝


按照剪枝类型进行划分，剪枝分为非结构化剪枝和结构化剪枝。
- 非结构化剪枝：修剪参数的单个元素。
- 结构化剪枝：有选择性地移除网络的较大部分，比如一层或一个通道。


### 非结构化剪枝

非结构化剪枝去除不重要的神经元，相应地，被剪除的神经元和其他神经元之间的连接在计算时会被忽略。由于剪枝后的模型通常很稀疏，并且破坏了原有模型的结构，所以这类方法被称为非结构化剪枝。非结构化剪枝能极大降低模型的参数量和理论计算量，但是现有硬件架构的计算方式无法对其进行加速，所以在实际运行速度上得不到提升，需要设计特定的硬件才可能加速。

### 结构化剪枝

通常以filter或者整个网络层为基本单位进行剪枝。一个filter被剪枝，那么其前一个特征图和下一个特征图都会发生相应的变化，但是模型的结构却没有被破坏，仍然能够通过 GPU 或其他硬件来加速，因此这类方法被称之为结构化剪枝。


按照剪枝范围进行划分，剪枝分为局部剪枝和全局剪枝。
- 局部剪枝：
- 全局剪枝：

### 局部剪枝


### 全局剪枝


## 怎么剪枝（How to prune?）


### 基于权重大小


### 基于梯度大小



## 剪枝时机（When to prune?）

### 训练后剪枝


### 训练时剪枝



## 剪枝频率（How often?）


### 迭代（Iterative）

### One-Shot

