# 第1章 引言

介绍模型压缩的现状、具有的相关技术、适用层级及其优缺点

&emsp;&emsp;深度学习模型具有数百万（甚至数十亿）参数，通常包含冗余和不必要的连接。这些连接导致模型尺寸较大、推理速度较慢且内存需求较高。为了解决这个问题，研究人员开发了剪枝、量化、蒸馏与神经网络架构搜索等模型压缩技术。这些模型压缩技术有助于解决现代神经网络日益增长的复杂性和资源需求所带来的挑战。通过减小模型大小和提高效率，使得模型可以在各种设备上部署深度学习模型，为跨各个领域的实际应用提供可能性。

## 剪枝

&emsp;&emsp;剪枝主要是从深度学习模型中识别和删除不必要的连接、权重甚至整个神经元。通过消除这些冗余组件，模型可以变得更紧凑、更快、内存效率更高，同时仍保持较高的准确性。一般建议剪枝从修剪权重开始，因为这不会像修剪神经元那样改变模型的架构。修剪权重的本质是将网络中所选单个参数的权重设置为零。这些参数将无法影响模型的推理。

## 量化

&emsp;&emsp;量化是另一种通过降低权重和激活的精度来使神经网络更小、更快、更高效的技术。在传统的深度学习模型中，权重和偏差等参数通常使用 32 位浮点数（单精度）进行存储和处理，这提供了高精度，但需要大量的内存和计算资源。量化通过使用较少位数（例如 8 位或甚至更低）表示这些值来解决此问题。这减少了模型的内存占用，并通过利用整数运算的硬件优化来加速计算。

## 蒸馏

&emsp;&emsp;蒸馏是一种用于将知识从大型复杂模型（通常称为教师模型）转移到较小的简化模型（称为学生模型）的技术。教师模型包含在大型数据集训练过程中学到的大量信息。蒸馏旨在将这些知识提炼成更紧凑、更高效的形式，可以轻松部署在资源受限的设备上或计算能力有限的场景中。

## 神经网络架构搜索



## 总结


