# Awesome Compression

## 项目简介

&emsp;&emsp;随着ChatGPT的出圈，大语言模型层出不穷，并展现出非凡的能力，可以有效地解决各种复杂问题。然而，这些模型通常需要大量的计算资源和内存，导致运行时资源消耗较高，限制了其在某些场景下的应用，让很多研究者望而却步。本项目使用通俗易懂的语言介绍模型的剪枝、量化、知识蒸馏等压缩方法，让更多的小白能更快了解到模型压缩技术。

## 项目意义

&emsp;&emsp;目前网上关于模型压缩的相关资料比较驳杂，初学者很难找到一个简单优质的的中文入门教程来学习。本项目借鉴[MIT 6.5940 TinyML and Efficient Deep Learning Computing](https://hanlab.mit.edu/courses/2023-fall-65940)，提供模型压缩的入门教程，降低模型压缩的学习门槛。在教程中，您将了解不同的压缩方法，通过实践和示例，学习如何应用这些方法来压缩深度学习模型，以满足实际应用需求。


## 项目受众

&emsp;&emsp;本项目适合以下学习者：

- 深度学习研究人员；
- 嵌入式系统和移动应用开发者；
- 对AI硬件加速和部署感兴趣的开发者；
- 对模型压缩技术感兴趣的学生群体。

## 项目亮点

- 提供通俗易懂的理论内容来科普模型压缩技术；
- 提供实践代码，结合实际场景帮助学习者更好地理解理论内容。

## 项目规划

### 目录

- 第1章 介绍
- 第2章 深度学习基础
- 第3章 模型剪枝
- 第4章 模型量化
- 第5章 神经架构搜索
- 第6章 知识蒸馏
- 第7章 MCUNet
- 第8章 TinyEngine
- 第9章 Transformer and LLM
- 第10章 Vision Transformer
- 第11章 GAN, Video, and Point Cloud
- 第12章 扩散模型
- 第13章 分布式训练
- 第14章 设备训练
- 第15章 量子计算基础
- 第16章 Noise Robust Quantum ML
- 第17章 项目实践


### 各章节负责人

- [陈玉立](https://github.com/ironartisan) (Datawhale成员-北京邮电大学研究生)
- [姜蔚蔚](https://github.com/jwwthu) (北京邮电大学助理教授)
- [施怡惠]()(北京邮电大学研究生)


### 各章节预估完成日期

- Step 1: 搭建内容框架并确认各章节负责人（1个月）；
- Step 2: 对章节内容进行撰写（3个月）；
- Step 3: 对整体内容进行修订与完善（1个月）。

## 环境安装
### Node.js版本

Node v16

### 安装docsify
```shell
npm i docsify-cli -g
```


### 启动docsify
```shell
docsify serve ./docs
```

## 课程知识思维导图





